_:N7d1f316bacb64fe9a62ef98f27686ca9 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/9730dc9a9eebf37a00a9e7ed19b0dd51b9f5f9cd6cf64d68e2eea9304e8a1bb5> .
<http://example.org/course/UE_Inconnu> <http://example.org/has_schema_type> _:N7d1f316bacb64fe9a62ef98f27686ca9 <http://groq.org/9730dc9a9eebf37a00a9e7ed19b0dd51b9f5f9cd6cf64d68e2eea9304e8a1bb5> .
_:N7d1f316bacb64fe9a62ef98f27686ca9 <http://schema.org/explain> "The lecture covers the core concepts of ethics in research, including ethical theories, principles, and frameworks, as well as the importance of cultural context and critical thinking." <http://groq.org/9730dc9a9eebf37a00a9e7ed19b0dd51b9f5f9cd6cf64d68e2eea9304e8a1bb5> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Research methodology and case study Objectif: (résultats d'apprentissage)\nguidelines\nof the teaching unit to their work. Course content: Ethics of research Course name: http://example.org/course/UE_Inconnu</page1>\n        <page2>Pages : 212-213-214 CS Core : 2, KA Core : 1 Ethical theories and principles\nare the foundations of ethical analysis because they are the viewpoints which\ncan provide guidance along the pathway to a decision. Each theory emphasizes\ndifferent assumptions and methods for determining the ethicality of a given\naction. It is important for students to recognize that decisions in different\ncontexts may require different ethical theories (including combinations) to\narrive at ethically acceptable outcomes, and what constitutes 'acceptable'\ndepends on a variety of factors such as cultural context. Applying methods for\nethical analysis requires both an understanding of the underlying principles\nand assumptions guiding a given tool and an awareness of the social context\nfor that decision. Traditional ethical frameworks (e.g., [11]) as provided by\nwestern philosophy can be useful, but they are not all-inclusive. Effort must\nbe taken to include decolonial, indigenous, and historically marginalized\nethical perspectives whenever possible. No theory will be universally\napplicable to all contexts, nor is any single ethical framework the 'best.'\nEngagement across various ethical schools of thought is important for students\nto develop the critical thinking needed in judiciously applying methods for\nethical analysis of a given situation. CS Core: 1\\. Avoiding fallacies and\nmisrepresentation in argumentation 2\\. Ethical theories and decision-making\n(philosophical and social frameworks, e.g. [1]) 3\\. Recognition of the role\nculture plays in our understanding, adoption, design, and use of computing\ntechnology 4\\. Why ethics is important in computing, and how ethics is similar\nto, and different from, laws andsocial norms KA Core: 5\\. Professional\nchecklists 6\\. Evaluation rubrics 7\\. Stakeholder analysis 8\\. Standpoint\ntheory 9\\. Introduction to ethical frameworks (e.g., consequentialism such as\nutilitarianism, non-consequentialism such as duty, rights, or justice, agent-\ncentered such as virtue or feminism, contractarianism, ethics of care) and\ntheir use for analyzing an ethical dilemma Illustrative Learning Outcomes: CS\nCore: 1\\. Describe how a given cultural context impacts decision making. 2\\.\nExpress the use of example and analogy in ethical argument. 3\\. Analyze (and\navoid) basic logical fallacies in an argument. 4\\. Analyze an argument to\nidentify premises and conclusion. 5\\. Evaluate how and why ethics is so\nimportant in computing and how it relates to cultural norms, values, and law.\n6\\. Justify a decision made on ethical grounds. KA Core: 7\\. Distinguish all\nstakeholder positions in relation to their cultural context in a given\nsituation. 8\\. Analyze the potential for introducing or perpetuating ethical\ndebt (deferred consideration of ethical impacts or implications) in technical\ndecisions. 9\\. Discuss the advantages and disadvantages of traditional ethical\nframeworks. 10\\. Analyze ethical dilemmas related to the creation and use of\ntechnology from multiple perspectives using ethical frameworks.</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/9730dc9a9eebf37a00a9e7ed19b0dd51b9f5f9cd6cf64d68e2eea9304e8a1bb5> .
<http://example.org/GROK> <http://example.org/param_model> "llama3-8b-8192" <http://groq.org/9730dc9a9eebf37a00a9e7ed19b0dd51b9f5f9cd6cf64d68e2eea9304e8a1bb5> .
_:N7d1f316bacb64fe9a62ef98f27686ca9 <http://schema.org/answer> "1" <http://groq.org/9730dc9a9eebf37a00a9e7ed19b0dd51b9f5f9cd6cf64d68e2eea9304e8a1bb5> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_Inconnu" <http://groq.org/9730dc9a9eebf37a00a9e7ed19b0dd51b9f5f9cd6cf64d68e2eea9304e8a1bb5> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Research methodology and case study Objectif: (résultats d'apprentissage)\nguidelines\nof the teaching unit to their work. Course content: Ethics of research Course name: http://example.org/course/UE_Inconnu</page1>\n        <page2>Pages:71-75 HOURS CS Core = 4 KA Core = 6 AI-ML: Machine Learning CS Core: 1\\.\nDefinition and examples of a broad variety of machine learning tasks a.\nSupervised learning i. Classification ii. Regression b. Reinforcement learning\nc. Unsupervised learning i. Clustering 2\\. Fundamental ideas: a. No free lunch\ntheorem: no one learner can solve all problems; representational design\ndecisions have consequences. b. Sources of error and undecidability in machine\nlearning 3\\. A simple statistical-based supervised learning such as linear\nregression or decision trees a. Focus on how they work without going into\nmathematical or optimization details; enough to understand and use existing\nimplementations correctly 4\\. The overfitting problem/controlling solution\ncomplexity (regularization, pruning - intuition only) a. The bias\n(underfitting) - variance (overfitting) tradeoff 5\\. Working with Data a. Data\npreprocessing i. Importance and pitfalls of preprocessing choices b. Handling\nmissing values (imputing, flag-as-missing) i. Implications of imputing vs\nflag-as-missing c. Encoding categorical variables, encoding real-valued data\nd. Normalization/standardization e. Emphasis on real data, not textbook\nexamples 6\\. Representations a. Hypothesis spaces and complexity b. Simple\nbasis feature expansion, such as squaring univariate features c. Learned\nfeature representations 7\\. Machine learning evaluation a. Separation of\ntrain, validation, and test sets b. Performance metrics for classifiers c.\nEstimation of test performance on held-out data d. Tuning the parameters of a\nmachine learning model with a validation set e. Importance of understanding\nwhat a model is doing, where its pitfalls/shortcomings are, and the\nimplications of its decisions 8\\. Basic neural networks a. Fundamentals of\nunderstanding how neural networks work and their training process, without\ndetails of the calculations b. Basic introduction to generative neural\nnetworks (e.g., large language models) 9\\. Ethics for Machine Learning (See\nalso: SEP-Context) a. Focus on real data, real scenarios, and case studies b.\nDataset/algorithmic/evaluation bias and unintended consequences 72 KA Core:\n10\\. Formulation of simple machine learning as an optimization problem, such\nas least squares linear regression or logistic regression a. Objective\nfunction b. Gradient descent c. Regularization to avoid overfitting\n(mathematical formulation) 11\\. Ensembles of models a. Simple weighted\nmajority combination 12\\. Deep learning a. Deep feed-forward networks\n(intuition only, no mathematics) b. Convolutional neural networks (intuition\nonly, no mathematics) c. Visualization of learned feature representations from\ndeep nets d. Other architectures (generative NN, recurrent NN, transformers,\netc.) 13\\. Performance evaluation a. Other metrics for classification (e.g.,\nerror, precision, recall) b. Performance metrics for regressors c. Confusion\nmatrix d. Cross-validation i. Parameter tuning (grid/random search, via cross-\nvalidation) 14\\. Overview of reinforcement learning methods 15\\. Two or more\napplications of machine learning algorithms a. E.g., medicine and health,\neconomics, vision, natural language, robotics, game play 16\\. Ethics for\nMachine Learning a. Continued focus on real data, real scenarios, and case\nstudies (See also: SEP-Context) b. Privacy (See also: SEP-Privacy) c. Fairness\n(See also: SEP-Privacy) d. Intellectual property e. Explainability Non-core:\n17\\. General statistical-based learning, parameter estimation (maximum\nlikelihood) 18\\. Supervised learning a. Decision trees b. Nearest-neighbor\nclassification and regression c. Learning simple neural networks / multi-layer\nperceptrons d. Linear regression e. Logistic regression f. Support vector\nmachines (SVMs) and kernels g. Gaussian Processes 19\\. Overfitting a. The\ncurse of dimensionality b. Regularization (mathematical computations, L2 and\nL1 regularization) 20\\. Experimental design 73 a. Data preparation (e.g.,\nstandardization, representation, one-hot encoding) b. Hypothesis space c.\nBiases (e.g., algorithmic, search) d. Partitioning data: stratification,\ntraining set, validation set, test set e. Parameter tuning (grid/random\nsearch, via cross-validation) f. Performance evaluation i. Cross-validation\nii. Metric: error, precision, recall, confusion matrix iii. Receiver operating\ncharacteristic (ROC) curve and area under ROC curve 21\\. Bayesian learning\n(Cross-Reference AI/Reasoning Under Uncertainty) a. Naive Bayes and its\nrelationship to linear models b. Bayesian networks c. Prior/posterior d.\nGenerative models 22\\. Deep learning a. Deep feed-forward networks b. Neural\ntangent kernel and understanding neural network training c. Convolutional\nneural networks d. Autoencoders e. Recurrent networks f. Representations and\nknowledge transfer g. Adversarial training and generative adversarial networks\nh. Attention mechanisms 23\\. Representations a. Manually crafted\nrepresentations b. Basis expansion c. Learned representations (e.g., deep\nneural networks) 24\\. Unsupervised learning and clustering a. K-means b.\nGaussian mixture models c. Expectation maximization (EM) d. Self-organizing\nmaps 25\\. Graph analysis (e.g., PageRank) 26\\. Semi-supervised learning 27\\.\nGraphical models (See also: AI-Probability) 28\\. Ensembles a. Weighted\nmajority b. Boosting/bagging c. Random forest d. Gated ensemble 29\\. Learning\ntheory a. General overview of learning theory / why learning works b. VC\ndimension c. Generalization bounds 74 30\\. Reinforcement learning a.\nExploration vs exploitation tradeoff b. Markov decision processes c. Value and\npolicy iteration d. Policy gradient methods e. Deep reinforcement learning f.\nLearning from demonstration and inverse RL 31\\. Explainable / interpretable\nmachine learning a. Understanding feature importance (e.g., LIME, Shapley\nvalues) b. Interpretable models and representations 32\\. Recommender systems\n33\\. Hardware for machine learning a. GPUs / TPUs 34\\. Application of machine\nlearning algorithms to: a. Medicine and health b. Economics c. Education d.\nVision e. Natural language f. Robotics g. Game play h. Data mining (Cross-\nreference DM/Data Analytics) 35\\. Ethics for Machine Learning a. Continued\nfocus on real data, real scenarios, and case studies (See also: SEP-Context)\nb. In depth exploration of dataset/algorithmic/evaluation bias, data privacy,\nand fairness (See also: SEP-Privacy, SEP-Context) c. Trust / explainability\nIllustrative Learning Outcomes: 1\\. Describe the differences among the three\nmain styles of learning (supervised, reinforcement, and unsupervised) and\ndetermine which is appropriate to a particular problem domain. 2\\.\nDifferentiate the terms of AI, machine learning, and deep learning. 3\\. Frame\nan application as a classification problem, including the available input\nfeatures and output to be predicted (e.g., identifying alphabetic characters\nfrom pixel grid input). 4\\. Apply two or more simple statistical learning\nalgorithms to a classification task and measure the classifiers' accuracy. 5\\.\nIdentify overfitting in the context of a problem and learning curves and\ndescribe solutions to overfitting. 6\\. Explain how machine learning works as\nan optimization/search process. 7\\. Implement a statistical learning algorithm\nand the corresponding optimization process to train the classifier and obtain\na prediction on new data. 8\\. Describe the neural network training process and\nresulting learned representations. 75 9\\. Explain proper ML evaluation\nprocedures, including the differences between training and testing\nperformance, and what can go wrong with the evaluation process leading to\ninaccurate reporting of ML performance. 10\\. Compare two machine learning\nalgorithms on a dataset, implementing the data preprocessing and evaluation\nmethodology (e.g., metrics and handling of train/test splits) from scratch.\n11\\. Visualize the training progress of a neural network through learning\ncurves in a well-established toolkit (e.g., TensorBoard) and visualize the\nlearned features of the network. 12\\. Compare and contrast several learning\ntechniques (e.g., decision trees, logistic regression, naive Bayes, neural\nnetworks, and belief networks), providing examples of when each strategy is\nsuperior. 13\\. Evaluate the performance of a simple learning system on a real-\nworld dataset. 14\\. Characterize the state of the art in learning theory,\nincluding its achievements and shortcomings. 15\\. Explain the problem of\noverfitting, along with techniques for detecting and managing the problem.\n16\\. Explain the triple tradeoff among the size of a hypothesis space, the\nsize of the training set, and performance accuracy. 17\\. Given a real-world\napplication of machine learning, describe ethical issues regarding the choices\nof data, preprocessing steps, algorithm selection, and\nvisualization/presentation of results</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/1f1d14cdd1bd7be01f156f162e3328ab45e64a9dee1175694b7d596427b9feab> .
_:N5e92d23c66c2429095bb6609f492268f <http://schema.org/answer> "0" <http://groq.org/1f1d14cdd1bd7be01f156f162e3328ab45e64a9dee1175694b7d596427b9feab> .
<http://example.org/course/UE_Inconnu> <http://example.org/has_schema_type> _:N5e92d23c66c2429095bb6609f492268f <http://groq.org/1f1d14cdd1bd7be01f156f162e3328ab45e64a9dee1175694b7d596427b9feab> .
_:N5e92d23c66c2429095bb6609f492268f <http://schema.org/explain> "The lecture focuses on research methodology and ethics, which is a small part of the broader KU." <http://groq.org/1f1d14cdd1bd7be01f156f162e3328ab45e64a9dee1175694b7d596427b9feab> .
_:N5e92d23c66c2429095bb6609f492268f <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/1f1d14cdd1bd7be01f156f162e3328ab45e64a9dee1175694b7d596427b9feab> .
<http://example.org/GROK> <http://example.org/param_model> "deepseek-r1-distill-llama-70b" <http://groq.org/1f1d14cdd1bd7be01f156f162e3328ab45e64a9dee1175694b7d596427b9feab> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_Inconnu" <http://groq.org/1f1d14cdd1bd7be01f156f162e3328ab45e64a9dee1175694b7d596427b9feab> .
_:N6ea5f4a9a99f4724a26ff0024d2b27f7 <http://schema.org/answer> "0" <http://groq.org/9a2ccbcecf73b6f31287ec9f5d2581d3da9188cb1b9aac05b5bb556d0dfd2446> .
<http://example.org/GROK> <http://example.org/param_model> "qwen-qwq-32b" <http://groq.org/9a2ccbcecf73b6f31287ec9f5d2581d3da9188cb1b9aac05b5bb556d0dfd2446> .
_:N6ea5f4a9a99f4724a26ff0024d2b27f7 <http://schema.org/explain> "Lecture focuses on research ethics basics, while KU requires deeper analysis of multiple frameworks, cultural contexts, and specific methodologies not explicitly covered." <http://groq.org/9a2ccbcecf73b6f31287ec9f5d2581d3da9188cb1b9aac05b5bb556d0dfd2446> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Research methodology and case study Objectif: (résultats d'apprentissage)\nguidelines\nof the teaching unit to their work. Course content: Ethics of research Course name: http://example.org/course/UE_Inconnu</page1>\n        <page2>Pages : 212-213-214 CS Core : 2, KA Core : 1 Ethical theories and principles\nare the foundations of ethical analysis because they are the viewpoints which\ncan provide guidance along the pathway to a decision. Each theory emphasizes\ndifferent assumptions and methods for determining the ethicality of a given\naction. It is important for students to recognize that decisions in different\ncontexts may require different ethical theories (including combinations) to\narrive at ethically acceptable outcomes, and what constitutes 'acceptable'\ndepends on a variety of factors such as cultural context. Applying methods for\nethical analysis requires both an understanding of the underlying principles\nand assumptions guiding a given tool and an awareness of the social context\nfor that decision. Traditional ethical frameworks (e.g., [11]) as provided by\nwestern philosophy can be useful, but they are not all-inclusive. Effort must\nbe taken to include decolonial, indigenous, and historically marginalized\nethical perspectives whenever possible. No theory will be universally\napplicable to all contexts, nor is any single ethical framework the 'best.'\nEngagement across various ethical schools of thought is important for students\nto develop the critical thinking needed in judiciously applying methods for\nethical analysis of a given situation. CS Core: 1\\. Avoiding fallacies and\nmisrepresentation in argumentation 2\\. Ethical theories and decision-making\n(philosophical and social frameworks, e.g. [1]) 3\\. Recognition of the role\nculture plays in our understanding, adoption, design, and use of computing\ntechnology 4\\. Why ethics is important in computing, and how ethics is similar\nto, and different from, laws andsocial norms KA Core: 5\\. Professional\nchecklists 6\\. Evaluation rubrics 7\\. Stakeholder analysis 8\\. Standpoint\ntheory 9\\. Introduction to ethical frameworks (e.g., consequentialism such as\nutilitarianism, non-consequentialism such as duty, rights, or justice, agent-\ncentered such as virtue or feminism, contractarianism, ethics of care) and\ntheir use for analyzing an ethical dilemma Illustrative Learning Outcomes: CS\nCore: 1\\. Describe how a given cultural context impacts decision making. 2\\.\nExpress the use of example and analogy in ethical argument. 3\\. Analyze (and\navoid) basic logical fallacies in an argument. 4\\. Analyze an argument to\nidentify premises and conclusion. 5\\. Evaluate how and why ethics is so\nimportant in computing and how it relates to cultural norms, values, and law.\n6\\. Justify a decision made on ethical grounds. KA Core: 7\\. Distinguish all\nstakeholder positions in relation to their cultural context in a given\nsituation. 8\\. Analyze the potential for introducing or perpetuating ethical\ndebt (deferred consideration of ethical impacts or implications) in technical\ndecisions. 9\\. Discuss the advantages and disadvantages of traditional ethical\nframeworks. 10\\. Analyze ethical dilemmas related to the creation and use of\ntechnology from multiple perspectives using ethical frameworks.</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/9a2ccbcecf73b6f31287ec9f5d2581d3da9188cb1b9aac05b5bb556d0dfd2446> .
_:N6ea5f4a9a99f4724a26ff0024d2b27f7 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/9a2ccbcecf73b6f31287ec9f5d2581d3da9188cb1b9aac05b5bb556d0dfd2446> .
<http://example.org/course/UE_Inconnu> <http://example.org/has_schema_type> _:N6ea5f4a9a99f4724a26ff0024d2b27f7 <http://groq.org/9a2ccbcecf73b6f31287ec9f5d2581d3da9188cb1b9aac05b5bb556d0dfd2446> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_Inconnu" <http://groq.org/9a2ccbcecf73b6f31287ec9f5d2581d3da9188cb1b9aac05b5bb556d0dfd2446> .
_:N0f0c8a6d75f94fbc9d74e432a7ae562a <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/79f2db14c244e763b372f331fd84135ae9bcc278e10cd0616c3f7436be67d985> .
<http://example.org/GROK> <http://example.org/param_model> "deepseek-r1-distill-llama-70b" <http://groq.org/79f2db14c244e763b372f331fd84135ae9bcc278e10cd0616c3f7436be67d985> .
<http://example.org/course/UE_Inconnu> <http://example.org/has_schema_type> _:N0f0c8a6d75f94fbc9d74e432a7ae562a <http://groq.org/79f2db14c244e763b372f331fd84135ae9bcc278e10cd0616c3f7436be67d985> .
_:N0f0c8a6d75f94fbc9d74e432a7ae562a <http://schema.org/explain> "The lecture covers ethics but lacks the breadth and depth of the KU." <http://groq.org/79f2db14c244e763b372f331fd84135ae9bcc278e10cd0616c3f7436be67d985> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Research methodology and case study Objectif: (résultats d'apprentissage)\nguidelines\nof the teaching unit to their work. Course content: Ethics of research Course name: http://example.org/course/UE_Inconnu</page1>\n        <page2>Pages : 212-213-214 CS Core : 2, KA Core : 1 Ethical theories and principles\nare the foundations of ethical analysis because they are the viewpoints which\ncan provide guidance along the pathway to a decision. Each theory emphasizes\ndifferent assumptions and methods for determining the ethicality of a given\naction. It is important for students to recognize that decisions in different\ncontexts may require different ethical theories (including combinations) to\narrive at ethically acceptable outcomes, and what constitutes 'acceptable'\ndepends on a variety of factors such as cultural context. Applying methods for\nethical analysis requires both an understanding of the underlying principles\nand assumptions guiding a given tool and an awareness of the social context\nfor that decision. Traditional ethical frameworks (e.g., [11]) as provided by\nwestern philosophy can be useful, but they are not all-inclusive. Effort must\nbe taken to include decolonial, indigenous, and historically marginalized\nethical perspectives whenever possible. No theory will be universally\napplicable to all contexts, nor is any single ethical framework the 'best.'\nEngagement across various ethical schools of thought is important for students\nto develop the critical thinking needed in judiciously applying methods for\nethical analysis of a given situation. CS Core: 1\\. Avoiding fallacies and\nmisrepresentation in argumentation 2\\. Ethical theories and decision-making\n(philosophical and social frameworks, e.g. [1]) 3\\. Recognition of the role\nculture plays in our understanding, adoption, design, and use of computing\ntechnology 4\\. Why ethics is important in computing, and how ethics is similar\nto, and different from, laws andsocial norms KA Core: 5\\. Professional\nchecklists 6\\. Evaluation rubrics 7\\. Stakeholder analysis 8\\. Standpoint\ntheory 9\\. Introduction to ethical frameworks (e.g., consequentialism such as\nutilitarianism, non-consequentialism such as duty, rights, or justice, agent-\ncentered such as virtue or feminism, contractarianism, ethics of care) and\ntheir use for analyzing an ethical dilemma Illustrative Learning Outcomes: CS\nCore: 1\\. Describe how a given cultural context impacts decision making. 2\\.\nExpress the use of example and analogy in ethical argument. 3\\. Analyze (and\navoid) basic logical fallacies in an argument. 4\\. Analyze an argument to\nidentify premises and conclusion. 5\\. Evaluate how and why ethics is so\nimportant in computing and how it relates to cultural norms, values, and law.\n6\\. Justify a decision made on ethical grounds. KA Core: 7\\. Distinguish all\nstakeholder positions in relation to their cultural context in a given\nsituation. 8\\. Analyze the potential for introducing or perpetuating ethical\ndebt (deferred consideration of ethical impacts or implications) in technical\ndecisions. 9\\. Discuss the advantages and disadvantages of traditional ethical\nframeworks. 10\\. Analyze ethical dilemmas related to the creation and use of\ntechnology from multiple perspectives using ethical frameworks.</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/79f2db14c244e763b372f331fd84135ae9bcc278e10cd0616c3f7436be67d985> .
_:N0f0c8a6d75f94fbc9d74e432a7ae562a <http://schema.org/answer> "0" <http://groq.org/79f2db14c244e763b372f331fd84135ae9bcc278e10cd0616c3f7436be67d985> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_Inconnu" <http://groq.org/79f2db14c244e763b372f331fd84135ae9bcc278e10cd0616c3f7436be67d985> .
_:N33aa129b215a435b825d5cd3f27c467f <http://example.org/has_score> "0.7026297"^^<http://www.w3.org/2001/XMLSchema#float> <http://faiss.org/ae2285a4e3043c4d770fd38ac0ca207a5014d6d65da99c613f780ea984fd3688> .
<http://example.org/faiss> <http://example.org/input> "Label: Research methodology and case study Objectif: (résultats d'apprentissage)\nguidelines\nof the teaching unit to their work. Course content: Ethics of research Course name: http://example.org/course/UE_Inconnu" <http://faiss.org/ae2285a4e3043c4d770fd38ac0ca207a5014d6d65da99c613f780ea984fd3688> .
_:N33aa129b215a435b825d5cd3f27c467f <http://example.org/has_chunk> ". Understanding ethical approval requirements before engaging in user research (See also: SETools, SEP-Ethical-Analysis, SEP-Security, SEP-Privacy, SEP-Professional-Ethics)\nKA Core:\n2. Methods for evaluation with users (See also: SE-Validation)\na. Qualitative methods (qualitative coding and thematic analysis)\nb. Quantitative methods (statistical tests)\nc. Mixed methods (e.g., observation, think-aloud, interview, survey, experiment)\nd. Presentation requirements (e.g., reports, personas)\ne" <http://faiss.org/ae2285a4e3043c4d770fd38ac0ca207a5014d6d65da99c613f780ea984fd3688> .
_:N244ee0c3a18443648353fc3ece4db31e <http://example.org/has_chunk> ". Ethics for Machine Learning\na. Continued focus on real data, real scenarios, and case studies (See also: SEP-Context)\nb. Privacy (See also: SEP-Privacy)\nc. Fairness (See also: SEP-Privacy)\nd. Intellectual property\ne. Explainability\nNon-core:\n17. General statistical-based learning, parameter estimation (maximum likelihood)\n18. Supervised learning\na. Decision trees\nb. Nearest-neighbor classification and regression\nc. Learning simple neural networks / multi-layer perceptrons\nd. Linear regression\ne" <http://faiss.org/ae2285a4e3043c4d770fd38ac0ca207a5014d6d65da99c613f780ea984fd3688> .
_:N244ee0c3a18443648353fc3ece4db31e <http://example.org/has_source> <file:///home/ilefm/SLM-EXP/data/BodyOfKnowledge/Artificial_Intelligence_AI/AI-ML_Machine_Learning.txt> <http://faiss.org/ae2285a4e3043c4d770fd38ac0ca207a5014d6d65da99c613f780ea984fd3688> .
_:N1e464378bdf04a5fbb89b3c330c2399b <http://example.org/has_score> "0.6924315"^^<http://www.w3.org/2001/XMLSchema#float> <http://faiss.org/ae2285a4e3043c4d770fd38ac0ca207a5014d6d65da99c613f780ea984fd3688> .
_:N1e464378bdf04a5fbb89b3c330c2399b <http://example.org/has_chunk> ". Stakeholder analysis \n8. Standpoint theory \n9. Introduction to ethical frameworks (e.g., consequentialism such as utilitarianism, non-consequentialism such as duty, rights, or justice, agent-centered such as virtue or feminism, contractarianism, ethics of care) and their use for analyzing an ethical dilemma \n \nIllustrative Learning Outcomes: \nCS Core: \n1. Describe how a given cultural context impacts decision making. \n2. Express the use of example and analogy in ethical argument.  \n3" <http://faiss.org/ae2285a4e3043c4d770fd38ac0ca207a5014d6d65da99c613f780ea984fd3688> .
<http://example.org/course/UE_Inconnu> <http://example.org/is_aligned_with> _:N244ee0c3a18443648353fc3ece4db31e <http://faiss.org/ae2285a4e3043c4d770fd38ac0ca207a5014d6d65da99c613f780ea984fd3688> .
<http://example.org/course/UE_Inconnu> <http://example.org/is_aligned_with> _:N33aa129b215a435b825d5cd3f27c467f <http://faiss.org/ae2285a4e3043c4d770fd38ac0ca207a5014d6d65da99c613f780ea984fd3688> .
_:N244ee0c3a18443648353fc3ece4db31e <http://example.org/has_score> "0.6832884"^^<http://www.w3.org/2001/XMLSchema#float> <http://faiss.org/ae2285a4e3043c4d770fd38ac0ca207a5014d6d65da99c613f780ea984fd3688> .
_:N33aa129b215a435b825d5cd3f27c467f <http://example.org/has_source> <file:///home/ilefm/SLM-EXP/data/BodyOfKnowledge/Human_Computer_Interaction_HCI/HCI-Evaluation_Evaluating_the_Design.txt> <http://faiss.org/ae2285a4e3043c4d770fd38ac0ca207a5014d6d65da99c613f780ea984fd3688> .
<http://example.org/course/UE_Inconnu> <http://example.org/is_aligned_with> _:N1e464378bdf04a5fbb89b3c330c2399b <http://faiss.org/ae2285a4e3043c4d770fd38ac0ca207a5014d6d65da99c613f780ea984fd3688> .
_:N1e464378bdf04a5fbb89b3c330c2399b <http://example.org/has_source> <file:///home/ilefm/SLM-EXP/data/BodyOfKnowledge/Society_Ethics_and_the_Profession_SEP/SEP-Methods_for_Ethical_Analysis.txt> <http://faiss.org/ae2285a4e3043c4d770fd38ac0ca207a5014d6d65da99c613f780ea984fd3688> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Research methodology and case study Objectif: (résultats d'apprentissage)\nguidelines\nof the teaching unit to their work. Course content: Ethics of research Course name: http://example.org/course/UE_Inconnu</page1>\n        <page2>Pages: 178-179 HOURS CS Core = 1 KA Core = 2 HCI-Evaluation: Evaluating the\nDesign CS Core: 1\\. Methods for evaluation with users a. Formative (e.g.,\nneeds-finding, exploratory analysis) and summative assessment (e.g.,\nfunctionality and usability testing) b. Elements to evaluate (e.g., utility,\nefficiency, learnability, user satisfaction, affective elements such as\npleasure and engagement) c. Understanding ethical approval requirements before\nengaging in user research (See also: SETools, SEP-Ethical-Analysis, SEP-\nSecurity, SEP-Privacy, SEP-Professional-Ethics) KA Core: 2\\. Methods for\nevaluation with users (See also: SE-Validation) a. Qualitative methods\n(qualitative coding and thematic analysis) b. Quantitative methods\n(statistical tests) c. Mixed methods (e.g., observation, think-aloud,\ninterview, survey, experiment) d. Presentation requirements (e.g., reports,\npersonas) e. User-centered testing f. Heuristic evaluation g. Challenges and\nshortcomings to effective evaluation (e.g., sampling, generalization) 3\\.\nStudy planning a. How to set study goals b. Hypothesis design c. Approvals\nfrom Institutional Research Boards and ethics committees (See also: SEP-\nEthicalAnalysis, SEP-Security, SEP-Privacy) d. How to pre-register a study e.\nWithin-subjects vs between-subjects design 4\\. Implications and impacts of\ndesign with respect to the environment, material, society, security, privacy,\nethics, and broader impacts. (See also: SEC-Foundations) a. The environment b.\nMaterial c. Society d. Security e. Privacy f. Ethics g. Broader impacts Non-\ncore: 5\\. Techniques and tools for quantitative analysis a. Statistical\npackages b. Visualization tools c. Statistical tests (e.g., ANOVA, t-tests,\npost-hoc analysis, parametric vs non-parametric tests) d. Data exploration and\nvisual analytics; how to calculate effect size. 6\\. Data management a. Data\nstorage and data sharing (open science) b. Sensitivity and identifiability.\nIllustrative Learning Outcomes: CS Core: 1\\. Discuss the differences between\nformative and summative assessment and their role in evaluating design KA\nCore: 2\\. Select appropriate formative or summative evaluation methods at\ndifferent points throughout the development of a design. 3\\. Discuss the\nbenefits of using both qualitative and quantitative methods for evaluation.\n4\\. Evaluate the implications and broader impacts of a given design. 5\\. Plan\na usability evaluation for a given user interface, and justify its study\ngoals, hypothesis design, and study design. 6\\. Conduct a usability evaluation\nof a given user interface and draw defensible conclusions given the study\ndesign. Non-core: 7\\. Select and run appropriate statistical tests on provided\nstudy data to test for significance in the results. 8\\. Pre-register a study\ndesign, with planned statistical tests</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/b0891409b3a9f9765c3b0a263ecaa440360846af5c57ff077360b3ae26150a64> .
<http://example.org/course/UE_Inconnu> <http://example.org/has_schema_type> _:N5a4c0a4081114480975f68532ed3e7fb <http://groq.org/b0891409b3a9f9765c3b0a263ecaa440360846af5c57ff077360b3ae26150a64> .
<http://example.org/GROK> <http://example.org/param_model> "llama3-8b-8192" <http://groq.org/b0891409b3a9f9765c3b0a263ecaa440360846af5c57ff077360b3ae26150a64> .
_:N5a4c0a4081114480975f68532ed3e7fb <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/b0891409b3a9f9765c3b0a263ecaa440360846af5c57ff077360b3ae26150a64> .
_:N5a4c0a4081114480975f68532ed3e7fb <http://schema.org/answer> "1" <http://groq.org/b0891409b3a9f9765c3b0a263ecaa440360846af5c57ff077360b3ae26150a64> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_Inconnu" <http://groq.org/b0891409b3a9f9765c3b0a263ecaa440360846af5c57ff077360b3ae26150a64> .
_:N5a4c0a4081114480975f68532ed3e7fb <http://schema.org/explain> "The lecture covers the core topics of research methodology and case study, including ethics of research, evaluation methods, and study planning, which aligns with the majority of the Knowledge Unit's content." <http://groq.org/b0891409b3a9f9765c3b0a263ecaa440360846af5c57ff077360b3ae26150a64> .
_:N14f19aa378494cfd90deb0dc6f8e7a3b <http://schema.org/explain> "The lecture covers a significant portion of the KU, including topics such as machine learning, supervised and unsupervised learning, neural networks, and ethics." <http://groq.org/6366bf6222e0964c8bbd4165c7480e0e623478c39d4680be1b1a2667716508ac> .
_:N14f19aa378494cfd90deb0dc6f8e7a3b <http://schema.org/answer> "1" <http://groq.org/6366bf6222e0964c8bbd4165c7480e0e623478c39d4680be1b1a2667716508ac> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Research methodology and case study Objectif: (résultats d'apprentissage)\nguidelines\nof the teaching unit to their work. Course content: Ethics of research Course name: http://example.org/course/UE_Inconnu</page1>\n        <page2>Pages:71-75 HOURS CS Core = 4 KA Core = 6 AI-ML: Machine Learning CS Core: 1\\.\nDefinition and examples of a broad variety of machine learning tasks a.\nSupervised learning i. Classification ii. Regression b. Reinforcement learning\nc. Unsupervised learning i. Clustering 2\\. Fundamental ideas: a. No free lunch\ntheorem: no one learner can solve all problems; representational design\ndecisions have consequences. b. Sources of error and undecidability in machine\nlearning 3\\. A simple statistical-based supervised learning such as linear\nregression or decision trees a. Focus on how they work without going into\nmathematical or optimization details; enough to understand and use existing\nimplementations correctly 4\\. The overfitting problem/controlling solution\ncomplexity (regularization, pruning - intuition only) a. The bias\n(underfitting) - variance (overfitting) tradeoff 5\\. Working with Data a. Data\npreprocessing i. Importance and pitfalls of preprocessing choices b. Handling\nmissing values (imputing, flag-as-missing) i. Implications of imputing vs\nflag-as-missing c. Encoding categorical variables, encoding real-valued data\nd. Normalization/standardization e. Emphasis on real data, not textbook\nexamples 6\\. Representations a. Hypothesis spaces and complexity b. Simple\nbasis feature expansion, such as squaring univariate features c. Learned\nfeature representations 7\\. Machine learning evaluation a. Separation of\ntrain, validation, and test sets b. Performance metrics for classifiers c.\nEstimation of test performance on held-out data d. Tuning the parameters of a\nmachine learning model with a validation set e. Importance of understanding\nwhat a model is doing, where its pitfalls/shortcomings are, and the\nimplications of its decisions 8\\. Basic neural networks a. Fundamentals of\nunderstanding how neural networks work and their training process, without\ndetails of the calculations b. Basic introduction to generative neural\nnetworks (e.g., large language models) 9\\. Ethics for Machine Learning (See\nalso: SEP-Context) a. Focus on real data, real scenarios, and case studies b.\nDataset/algorithmic/evaluation bias and unintended consequences 72 KA Core:\n10\\. Formulation of simple machine learning as an optimization problem, such\nas least squares linear regression or logistic regression a. Objective\nfunction b. Gradient descent c. Regularization to avoid overfitting\n(mathematical formulation) 11\\. Ensembles of models a. Simple weighted\nmajority combination 12\\. Deep learning a. Deep feed-forward networks\n(intuition only, no mathematics) b. Convolutional neural networks (intuition\nonly, no mathematics) c. Visualization of learned feature representations from\ndeep nets d. Other architectures (generative NN, recurrent NN, transformers,\netc.) 13\\. Performance evaluation a. Other metrics for classification (e.g.,\nerror, precision, recall) b. Performance metrics for regressors c. Confusion\nmatrix d. Cross-validation i. Parameter tuning (grid/random search, via cross-\nvalidation) 14\\. Overview of reinforcement learning methods 15\\. Two or more\napplications of machine learning algorithms a. E.g., medicine and health,\neconomics, vision, natural language, robotics, game play 16\\. Ethics for\nMachine Learning a. Continued focus on real data, real scenarios, and case\nstudies (See also: SEP-Context) b. Privacy (See also: SEP-Privacy) c. Fairness\n(See also: SEP-Privacy) d. Intellectual property e. Explainability Non-core:\n17\\. General statistical-based learning, parameter estimation (maximum\nlikelihood) 18\\. Supervised learning a. Decision trees b. Nearest-neighbor\nclassification and regression c. Learning simple neural networks / multi-layer\nperceptrons d. Linear regression e. Logistic regression f. Support vector\nmachines (SVMs) and kernels g. Gaussian Processes 19\\. Overfitting a. The\ncurse of dimensionality b. Regularization (mathematical computations, L2 and\nL1 regularization) 20\\. Experimental design 73 a. Data preparation (e.g.,\nstandardization, representation, one-hot encoding) b. Hypothesis space c.\nBiases (e.g., algorithmic, search) d. Partitioning data: stratification,\ntraining set, validation set, test set e. Parameter tuning (grid/random\nsearch, via cross-validation) f. Performance evaluation i. Cross-validation\nii. Metric: error, precision, recall, confusion matrix iii. Receiver operating\ncharacteristic (ROC) curve and area under ROC curve 21\\. Bayesian learning\n(Cross-Reference AI/Reasoning Under Uncertainty) a. Naive Bayes and its\nrelationship to linear models b. Bayesian networks c. Prior/posterior d.\nGenerative models 22\\. Deep learning a. Deep feed-forward networks b. Neural\ntangent kernel and understanding neural network training c. Convolutional\nneural networks d. Autoencoders e. Recurrent networks f. Representations and\nknowledge transfer g. Adversarial training and generative adversarial networks\nh. Attention mechanisms 23\\. Representations a. Manually crafted\nrepresentations b. Basis expansion c. Learned representations (e.g., deep\nneural networks) 24\\. Unsupervised learning and clustering a. K-means b.\nGaussian mixture models c. Expectation maximization (EM) d. Self-organizing\nmaps 25\\. Graph analysis (e.g., PageRank) 26\\. Semi-supervised learning 27\\.\nGraphical models (See also: AI-Probability) 28\\. Ensembles a. Weighted\nmajority b. Boosting/bagging c. Random forest d. Gated ensemble 29\\. Learning\ntheory a. General overview of learning theory / why learning works b. VC\ndimension c. Generalization bounds 74 30\\. Reinforcement learning a.\nExploration vs exploitation tradeoff b. Markov decision processes c. Value and\npolicy iteration d. Policy gradient methods e. Deep reinforcement learning f.\nLearning from demonstration and inverse RL 31\\. Explainable / interpretable\nmachine learning a. Understanding feature importance (e.g., LIME, Shapley\nvalues) b. Interpretable models and representations 32\\. Recommender systems\n33\\. Hardware for machine learning a. GPUs / TPUs 34\\. Application of machine\nlearning algorithms to: a. Medicine and health b. Economics c. Education d.\nVision e. Natural language f. Robotics g. Game play h. Data mining (Cross-\nreference DM/Data Analytics) 35\\. Ethics for Machine Learning a. Continued\nfocus on real data, real scenarios, and case studies (See also: SEP-Context)\nb. In depth exploration of dataset/algorithmic/evaluation bias, data privacy,\nand fairness (See also: SEP-Privacy, SEP-Context) c. Trust / explainability\nIllustrative Learning Outcomes: 1\\. Describe the differences among the three\nmain styles of learning (supervised, reinforcement, and unsupervised) and\ndetermine which is appropriate to a particular problem domain. 2\\.\nDifferentiate the terms of AI, machine learning, and deep learning. 3\\. Frame\nan application as a classification problem, including the available input\nfeatures and output to be predicted (e.g., identifying alphabetic characters\nfrom pixel grid input). 4\\. Apply two or more simple statistical learning\nalgorithms to a classification task and measure the classifiers' accuracy. 5\\.\nIdentify overfitting in the context of a problem and learning curves and\ndescribe solutions to overfitting. 6\\. Explain how machine learning works as\nan optimization/search process. 7\\. Implement a statistical learning algorithm\nand the corresponding optimization process to train the classifier and obtain\na prediction on new data. 8\\. Describe the neural network training process and\nresulting learned representations. 75 9\\. Explain proper ML evaluation\nprocedures, including the differences between training and testing\nperformance, and what can go wrong with the evaluation process leading to\ninaccurate reporting of ML performance. 10\\. Compare two machine learning\nalgorithms on a dataset, implementing the data preprocessing and evaluation\nmethodology (e.g., metrics and handling of train/test splits) from scratch.\n11\\. Visualize the training progress of a neural network through learning\ncurves in a well-established toolkit (e.g., TensorBoard) and visualize the\nlearned features of the network. 12\\. Compare and contrast several learning\ntechniques (e.g., decision trees, logistic regression, naive Bayes, neural\nnetworks, and belief networks), providing examples of when each strategy is\nsuperior. 13\\. Evaluate the performance of a simple learning system on a real-\nworld dataset. 14\\. Characterize the state of the art in learning theory,\nincluding its achievements and shortcomings. 15\\. Explain the problem of\noverfitting, along with techniques for detecting and managing the problem.\n16\\. Explain the triple tradeoff among the size of a hypothesis space, the\nsize of the training set, and performance accuracy. 17\\. Given a real-world\napplication of machine learning, describe ethical issues regarding the choices\nof data, preprocessing steps, algorithm selection, and\nvisualization/presentation of results</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/6366bf6222e0964c8bbd4165c7480e0e623478c39d4680be1b1a2667716508ac> .
<http://example.org/course/UE_Inconnu> <http://example.org/has_schema_type> _:N14f19aa378494cfd90deb0dc6f8e7a3b <http://groq.org/6366bf6222e0964c8bbd4165c7480e0e623478c39d4680be1b1a2667716508ac> .
<http://example.org/GROK> <http://example.org/param_model> "llama3-8b-8192" <http://groq.org/6366bf6222e0964c8bbd4165c7480e0e623478c39d4680be1b1a2667716508ac> .
_:N14f19aa378494cfd90deb0dc6f8e7a3b <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/6366bf6222e0964c8bbd4165c7480e0e623478c39d4680be1b1a2667716508ac> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_Inconnu" <http://groq.org/6366bf6222e0964c8bbd4165c7480e0e623478c39d4680be1b1a2667716508ac> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/evaluation> "Research methodology and case study 100%" <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/methods> "Langue d’enseignement Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/hours> "TOTAL : 17.5h Répartition : CM : 0h TD : 0h CI : 15h TP : 0h EAD : 2.5h" <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/objective> "(résultats d'apprentissage)\nguidelines\nof the teaching unit to their work." <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://www.w3.org/2000/01/rdf-schema#label> "Research methodology and case study" <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/code> "Inconnu" <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/prerequisite> "(s)" <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/parcours> "M2 Data Science (DS)" <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/responsible> "Volume horaire total TOTAL : 17.5h Répartition : CM : 0h TD : 0h CI : 15h TP : 0h EAD : 2.5h" <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/level> "Master" <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/bibliography> "Semantic knowledge representation" <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/semester> "3" <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/obtention> "Programme" <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/content> "Ethics of research" <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://example.org/course/UE> <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/location> "Polytech Nantes" <urn:x-rdflib:default> .
<http://example.org/course/UE_Inconnu> <http://example.org/course/language> "Anglais" <urn:x-rdflib:default> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Research methodology and case study Objectif: (résultats d'apprentissage)\nguidelines\nof the teaching unit to their work. Course content: Ethics of research Course name: http://example.org/course/UE_Inconnu</page1>\n        <page2>Pages:71-75 HOURS CS Core = 4 KA Core = 6 AI-ML: Machine Learning CS Core: 1\\.\nDefinition and examples of a broad variety of machine learning tasks a.\nSupervised learning i. Classification ii. Regression b. Reinforcement learning\nc. Unsupervised learning i. Clustering 2\\. Fundamental ideas: a. No free lunch\ntheorem: no one learner can solve all problems; representational design\ndecisions have consequences. b. Sources of error and undecidability in machine\nlearning 3\\. A simple statistical-based supervised learning such as linear\nregression or decision trees a. Focus on how they work without going into\nmathematical or optimization details; enough to understand and use existing\nimplementations correctly 4\\. The overfitting problem/controlling solution\ncomplexity (regularization, pruning - intuition only) a. The bias\n(underfitting) - variance (overfitting) tradeoff 5\\. Working with Data a. Data\npreprocessing i. Importance and pitfalls of preprocessing choices b. Handling\nmissing values (imputing, flag-as-missing) i. Implications of imputing vs\nflag-as-missing c. Encoding categorical variables, encoding real-valued data\nd. Normalization/standardization e. Emphasis on real data, not textbook\nexamples 6\\. Representations a. Hypothesis spaces and complexity b. Simple\nbasis feature expansion, such as squaring univariate features c. Learned\nfeature representations 7\\. Machine learning evaluation a. Separation of\ntrain, validation, and test sets b. Performance metrics for classifiers c.\nEstimation of test performance on held-out data d. Tuning the parameters of a\nmachine learning model with a validation set e. Importance of understanding\nwhat a model is doing, where its pitfalls/shortcomings are, and the\nimplications of its decisions 8\\. Basic neural networks a. Fundamentals of\nunderstanding how neural networks work and their training process, without\ndetails of the calculations b. Basic introduction to generative neural\nnetworks (e.g., large language models) 9\\. Ethics for Machine Learning (See\nalso: SEP-Context) a. Focus on real data, real scenarios, and case studies b.\nDataset/algorithmic/evaluation bias and unintended consequences 72 KA Core:\n10\\. Formulation of simple machine learning as an optimization problem, such\nas least squares linear regression or logistic regression a. Objective\nfunction b. Gradient descent c. Regularization to avoid overfitting\n(mathematical formulation) 11\\. Ensembles of models a. Simple weighted\nmajority combination 12\\. Deep learning a. Deep feed-forward networks\n(intuition only, no mathematics) b. Convolutional neural networks (intuition\nonly, no mathematics) c. Visualization of learned feature representations from\ndeep nets d. Other architectures (generative NN, recurrent NN, transformers,\netc.) 13\\. Performance evaluation a. Other metrics for classification (e.g.,\nerror, precision, recall) b. Performance metrics for regressors c. Confusion\nmatrix d. Cross-validation i. Parameter tuning (grid/random search, via cross-\nvalidation) 14\\. Overview of reinforcement learning methods 15\\. Two or more\napplications of machine learning algorithms a. E.g., medicine and health,\neconomics, vision, natural language, robotics, game play 16\\. Ethics for\nMachine Learning a. Continued focus on real data, real scenarios, and case\nstudies (See also: SEP-Context) b. Privacy (See also: SEP-Privacy) c. Fairness\n(See also: SEP-Privacy) d. Intellectual property e. Explainability Non-core:\n17\\. General statistical-based learning, parameter estimation (maximum\nlikelihood) 18\\. Supervised learning a. Decision trees b. Nearest-neighbor\nclassification and regression c. Learning simple neural networks / multi-layer\nperceptrons d. Linear regression e. Logistic regression f. Support vector\nmachines (SVMs) and kernels g. Gaussian Processes 19\\. Overfitting a. The\ncurse of dimensionality b. Regularization (mathematical computations, L2 and\nL1 regularization) 20\\. Experimental design 73 a. Data preparation (e.g.,\nstandardization, representation, one-hot encoding) b. Hypothesis space c.\nBiases (e.g., algorithmic, search) d. Partitioning data: stratification,\ntraining set, validation set, test set e. Parameter tuning (grid/random\nsearch, via cross-validation) f. Performance evaluation i. Cross-validation\nii. Metric: error, precision, recall, confusion matrix iii. Receiver operating\ncharacteristic (ROC) curve and area under ROC curve 21\\. Bayesian learning\n(Cross-Reference AI/Reasoning Under Uncertainty) a. Naive Bayes and its\nrelationship to linear models b. Bayesian networks c. Prior/posterior d.\nGenerative models 22\\. Deep learning a. Deep feed-forward networks b. Neural\ntangent kernel and understanding neural network training c. Convolutional\nneural networks d. Autoencoders e. Recurrent networks f. Representations and\nknowledge transfer g. Adversarial training and generative adversarial networks\nh. Attention mechanisms 23\\. Representations a. Manually crafted\nrepresentations b. Basis expansion c. Learned representations (e.g., deep\nneural networks) 24\\. Unsupervised learning and clustering a. K-means b.\nGaussian mixture models c. Expectation maximization (EM) d. Self-organizing\nmaps 25\\. Graph analysis (e.g., PageRank) 26\\. Semi-supervised learning 27\\.\nGraphical models (See also: AI-Probability) 28\\. Ensembles a. Weighted\nmajority b. Boosting/bagging c. Random forest d. Gated ensemble 29\\. Learning\ntheory a. General overview of learning theory / why learning works b. VC\ndimension c. Generalization bounds 74 30\\. Reinforcement learning a.\nExploration vs exploitation tradeoff b. Markov decision processes c. Value and\npolicy iteration d. Policy gradient methods e. Deep reinforcement learning f.\nLearning from demonstration and inverse RL 31\\. Explainable / interpretable\nmachine learning a. Understanding feature importance (e.g., LIME, Shapley\nvalues) b. Interpretable models and representations 32\\. Recommender systems\n33\\. Hardware for machine learning a. GPUs / TPUs 34\\. Application of machine\nlearning algorithms to: a. Medicine and health b. Economics c. Education d.\nVision e. Natural language f. Robotics g. Game play h. Data mining (Cross-\nreference DM/Data Analytics) 35\\. Ethics for Machine Learning a. Continued\nfocus on real data, real scenarios, and case studies (See also: SEP-Context)\nb. In depth exploration of dataset/algorithmic/evaluation bias, data privacy,\nand fairness (See also: SEP-Privacy, SEP-Context) c. Trust / explainability\nIllustrative Learning Outcomes: 1\\. Describe the differences among the three\nmain styles of learning (supervised, reinforcement, and unsupervised) and\ndetermine which is appropriate to a particular problem domain. 2\\.\nDifferentiate the terms of AI, machine learning, and deep learning. 3\\. Frame\nan application as a classification problem, including the available input\nfeatures and output to be predicted (e.g., identifying alphabetic characters\nfrom pixel grid input). 4\\. Apply two or more simple statistical learning\nalgorithms to a classification task and measure the classifiers' accuracy. 5\\.\nIdentify overfitting in the context of a problem and learning curves and\ndescribe solutions to overfitting. 6\\. Explain how machine learning works as\nan optimization/search process. 7\\. Implement a statistical learning algorithm\nand the corresponding optimization process to train the classifier and obtain\na prediction on new data. 8\\. Describe the neural network training process and\nresulting learned representations. 75 9\\. Explain proper ML evaluation\nprocedures, including the differences between training and testing\nperformance, and what can go wrong with the evaluation process leading to\ninaccurate reporting of ML performance. 10\\. Compare two machine learning\nalgorithms on a dataset, implementing the data preprocessing and evaluation\nmethodology (e.g., metrics and handling of train/test splits) from scratch.\n11\\. Visualize the training progress of a neural network through learning\ncurves in a well-established toolkit (e.g., TensorBoard) and visualize the\nlearned features of the network. 12\\. Compare and contrast several learning\ntechniques (e.g., decision trees, logistic regression, naive Bayes, neural\nnetworks, and belief networks), providing examples of when each strategy is\nsuperior. 13\\. Evaluate the performance of a simple learning system on a real-\nworld dataset. 14\\. Characterize the state of the art in learning theory,\nincluding its achievements and shortcomings. 15\\. Explain the problem of\noverfitting, along with techniques for detecting and managing the problem.\n16\\. Explain the triple tradeoff among the size of a hypothesis space, the\nsize of the training set, and performance accuracy. 17\\. Given a real-world\napplication of machine learning, describe ethical issues regarding the choices\nof data, preprocessing steps, algorithm selection, and\nvisualization/presentation of results</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/2f0ccd7a0a274875b310f05b60d538bab46a3e988c7f47035d812de1d7a51cc4> .
_:N348c3f72a77d450a8d814c10cf2eff62 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/2f0ccd7a0a274875b310f05b60d538bab46a3e988c7f47035d812de1d7a51cc4> .
<http://example.org/GROK> <http://example.org/param_model> "qwen-qwq-32b" <http://groq.org/2f0ccd7a0a274875b310f05b60d538bab46a3e988c7f47035d812de1d7a51cc4> .
_:N348c3f72a77d450a8d814c10cf2eff62 <http://schema.org/explain> "Lecture focuses on research ethics/methodology, while KU covers extensive ML technical content beyond ethics." <http://groq.org/2f0ccd7a0a274875b310f05b60d538bab46a3e988c7f47035d812de1d7a51cc4> .
_:N348c3f72a77d450a8d814c10cf2eff62 <http://schema.org/answer> "0" <http://groq.org/2f0ccd7a0a274875b310f05b60d538bab46a3e988c7f47035d812de1d7a51cc4> .
<http://example.org/course/UE_Inconnu> <http://example.org/has_schema_type> _:N348c3f72a77d450a8d814c10cf2eff62 <http://groq.org/2f0ccd7a0a274875b310f05b60d538bab46a3e988c7f47035d812de1d7a51cc4> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_Inconnu" <http://groq.org/2f0ccd7a0a274875b310f05b60d538bab46a3e988c7f47035d812de1d7a51cc4> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Research methodology and case study Objectif: (résultats d'apprentissage)\nguidelines\nof the teaching unit to their work. Course content: Ethics of research Course name: http://example.org/course/UE_Inconnu</page1>\n        <page2>Pages: 178-179 HOURS CS Core = 1 KA Core = 2 HCI-Evaluation: Evaluating the\nDesign CS Core: 1\\. Methods for evaluation with users a. Formative (e.g.,\nneeds-finding, exploratory analysis) and summative assessment (e.g.,\nfunctionality and usability testing) b. Elements to evaluate (e.g., utility,\nefficiency, learnability, user satisfaction, affective elements such as\npleasure and engagement) c. Understanding ethical approval requirements before\nengaging in user research (See also: SETools, SEP-Ethical-Analysis, SEP-\nSecurity, SEP-Privacy, SEP-Professional-Ethics) KA Core: 2\\. Methods for\nevaluation with users (See also: SE-Validation) a. Qualitative methods\n(qualitative coding and thematic analysis) b. Quantitative methods\n(statistical tests) c. Mixed methods (e.g., observation, think-aloud,\ninterview, survey, experiment) d. Presentation requirements (e.g., reports,\npersonas) e. User-centered testing f. Heuristic evaluation g. Challenges and\nshortcomings to effective evaluation (e.g., sampling, generalization) 3\\.\nStudy planning a. How to set study goals b. Hypothesis design c. Approvals\nfrom Institutional Research Boards and ethics committees (See also: SEP-\nEthicalAnalysis, SEP-Security, SEP-Privacy) d. How to pre-register a study e.\nWithin-subjects vs between-subjects design 4\\. Implications and impacts of\ndesign with respect to the environment, material, society, security, privacy,\nethics, and broader impacts. (See also: SEC-Foundations) a. The environment b.\nMaterial c. Society d. Security e. Privacy f. Ethics g. Broader impacts Non-\ncore: 5\\. Techniques and tools for quantitative analysis a. Statistical\npackages b. Visualization tools c. Statistical tests (e.g., ANOVA, t-tests,\npost-hoc analysis, parametric vs non-parametric tests) d. Data exploration and\nvisual analytics; how to calculate effect size. 6\\. Data management a. Data\nstorage and data sharing (open science) b. Sensitivity and identifiability.\nIllustrative Learning Outcomes: CS Core: 1\\. Discuss the differences between\nformative and summative assessment and their role in evaluating design KA\nCore: 2\\. Select appropriate formative or summative evaluation methods at\ndifferent points throughout the development of a design. 3\\. Discuss the\nbenefits of using both qualitative and quantitative methods for evaluation.\n4\\. Evaluate the implications and broader impacts of a given design. 5\\. Plan\na usability evaluation for a given user interface, and justify its study\ngoals, hypothesis design, and study design. 6\\. Conduct a usability evaluation\nof a given user interface and draw defensible conclusions given the study\ndesign. Non-core: 7\\. Select and run appropriate statistical tests on provided\nstudy data to test for significance in the results. 8\\. Pre-register a study\ndesign, with planned statistical tests</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/4234c90eca8e8b4526c1f239b17fcbd0609bf61ca64c57fb8c469eea1abc8179> .
_:Neaee7d2da36b40808e38fd9bcec2cca7 <http://schema.org/answer> "0" <http://groq.org/4234c90eca8e8b4526c1f239b17fcbd0609bf61ca64c57fb8c469eea1abc8179> .
<http://example.org/GROK> <http://example.org/param_model> "deepseek-r1-distill-llama-70b" <http://groq.org/4234c90eca8e8b4526c1f239b17fcbd0609bf61ca64c57fb8c469eea1abc8179> .
<http://example.org/course/UE_Inconnu> <http://example.org/has_schema_type> _:Neaee7d2da36b40808e38fd9bcec2cca7 <http://groq.org/4234c90eca8e8b4526c1f239b17fcbd0609bf61ca64c57fb8c469eea1abc8179> .
_:Neaee7d2da36b40808e38fd9bcec2cca7 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/4234c90eca8e8b4526c1f239b17fcbd0609bf61ca64c57fb8c469eea1abc8179> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_Inconnu" <http://groq.org/4234c90eca8e8b4526c1f239b17fcbd0609bf61ca64c57fb8c469eea1abc8179> .
_:Neaee7d2da36b40808e38fd9bcec2cca7 <http://schema.org/explain> "The lecture focuses on research ethics, which is a part of the KU, but the KU covers a broader range of topics including evaluation methods, study design, and statistical analysis, which are not addressed in the lecture." <http://groq.org/4234c90eca8e8b4526c1f239b17fcbd0609bf61ca64c57fb8c469eea1abc8179> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Research methodology and case study Objectif: (résultats d'apprentissage)\nguidelines\nof the teaching unit to their work. Course content: Ethics of research Course name: http://example.org/course/UE_Inconnu</page1>\n        <page2>Pages: 178-179 HOURS CS Core = 1 KA Core = 2 HCI-Evaluation: Evaluating the\nDesign CS Core: 1\\. Methods for evaluation with users a. Formative (e.g.,\nneeds-finding, exploratory analysis) and summative assessment (e.g.,\nfunctionality and usability testing) b. Elements to evaluate (e.g., utility,\nefficiency, learnability, user satisfaction, affective elements such as\npleasure and engagement) c. Understanding ethical approval requirements before\nengaging in user research (See also: SETools, SEP-Ethical-Analysis, SEP-\nSecurity, SEP-Privacy, SEP-Professional-Ethics) KA Core: 2\\. Methods for\nevaluation with users (See also: SE-Validation) a. Qualitative methods\n(qualitative coding and thematic analysis) b. Quantitative methods\n(statistical tests) c. Mixed methods (e.g., observation, think-aloud,\ninterview, survey, experiment) d. Presentation requirements (e.g., reports,\npersonas) e. User-centered testing f. Heuristic evaluation g. Challenges and\nshortcomings to effective evaluation (e.g., sampling, generalization) 3\\.\nStudy planning a. How to set study goals b. Hypothesis design c. Approvals\nfrom Institutional Research Boards and ethics committees (See also: SEP-\nEthicalAnalysis, SEP-Security, SEP-Privacy) d. How to pre-register a study e.\nWithin-subjects vs between-subjects design 4\\. Implications and impacts of\ndesign with respect to the environment, material, society, security, privacy,\nethics, and broader impacts. (See also: SEC-Foundations) a. The environment b.\nMaterial c. Society d. Security e. Privacy f. Ethics g. Broader impacts Non-\ncore: 5\\. Techniques and tools for quantitative analysis a. Statistical\npackages b. Visualization tools c. Statistical tests (e.g., ANOVA, t-tests,\npost-hoc analysis, parametric vs non-parametric tests) d. Data exploration and\nvisual analytics; how to calculate effect size. 6\\. Data management a. Data\nstorage and data sharing (open science) b. Sensitivity and identifiability.\nIllustrative Learning Outcomes: CS Core: 1\\. Discuss the differences between\nformative and summative assessment and their role in evaluating design KA\nCore: 2\\. Select appropriate formative or summative evaluation methods at\ndifferent points throughout the development of a design. 3\\. Discuss the\nbenefits of using both qualitative and quantitative methods for evaluation.\n4\\. Evaluate the implications and broader impacts of a given design. 5\\. Plan\na usability evaluation for a given user interface, and justify its study\ngoals, hypothesis design, and study design. 6\\. Conduct a usability evaluation\nof a given user interface and draw defensible conclusions given the study\ndesign. Non-core: 7\\. Select and run appropriate statistical tests on provided\nstudy data to test for significance in the results. 8\\. Pre-register a study\ndesign, with planned statistical tests</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/869e1edf05e317bffd53f293b09772d79abcf893160163708a33bcadedc10c85> .
<http://example.org/GROK> <http://example.org/param_model> "qwen-qwq-32b" <http://groq.org/869e1edf05e317bffd53f293b09772d79abcf893160163708a33bcadedc10c85> .
_:N47cb740b3fe1425e95ec5382dea4a44e <http://schema.org/answer> "0" <http://groq.org/869e1edf05e317bffd53f293b09772d79abcf893160163708a33bcadedc10c85> .
<http://example.org/course/UE_Inconnu> <http://example.org/has_schema_type> _:N47cb740b3fe1425e95ec5382dea4a44e <http://groq.org/869e1edf05e317bffd53f293b09772d79abcf893160163708a33bcadedc10c85> .
_:N47cb740b3fe1425e95ec5382dea4a44e <http://schema.org/explain> "Lecture focuses narrowly on research ethics, while KU covers broader evaluation methods, statistical analysis, and study design not addressed." <http://groq.org/869e1edf05e317bffd53f293b09772d79abcf893160163708a33bcadedc10c85> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_Inconnu" <http://groq.org/869e1edf05e317bffd53f293b09772d79abcf893160163708a33bcadedc10c85> .
_:N47cb740b3fe1425e95ec5382dea4a44e <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/869e1edf05e317bffd53f293b09772d79abcf893160163708a33bcadedc10c85> .

