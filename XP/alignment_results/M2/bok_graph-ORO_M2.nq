_:N695b301f759644b380a4aec3dc35342a <http://schema.org/explain> "The lecture covers topics in multi-objective combinatorial optimization, including e-constraint algorithm, weighted sum scalarization, and branch and bound algorithm, which aligns with the KU's focus on search algorithms and problem solving." <http://groq.org/b57bdcc944fe94b668634b72b05580ba3f2fe43695c2426fa9336bae9214b3f5> .
_:N695b301f759644b380a4aec3dc35342a <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/b57bdcc944fe94b668634b72b05580ba3f2fe43695c2426fa9336bae9214b3f5> .
_:N695b301f759644b380a4aec3dc35342a <http://schema.org/answer> "1" <http://groq.org/b57bdcc944fe94b668634b72b05580ba3f2fe43695c2426fa9336bae9214b3f5> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Optimisation multi-objectif Objectif: (résultats d'apprentissage)\nproblems (M)\n- To be able to apply the e-constraint algorithm with adaptive step to solve a bi-objective\ncombinatorial problem (A)\n- To be able to apply the weighted sum scalarization to compute a complete set of extreme\nsupported solution of a bi-objective combinatorial optimization problem (A)\n- To be able to define and update the search area of a bi-objective combinatorial optimization\nproblem (Phase 2 of the two phase method) (A)\n- To know the main strategies (variable fixing, ranking, branch and bound) to explore the search\narea (A)\n- To know the notion of bound sets (M)\n- To be able to apply a branch and bound algorithm to solve a bi-objective combinatorial\noptimization problem (A)\n- To be able to apply a dynamic programming algorithm for multi-objective combinatorial\noptimization problem, application to the shortest path problem (A) Course content: * Multi-objective combinatorial optimization - main definitions and properties\n* Multi-objective combinatorial optimization - main solution methods\n* Multi-objective shortest path problem Course name: http://example.org/course/UE_XMS3IU300</page1>\n        <page2>AI-Search: Search Pages: 68-70 HOURS CS Core = 5 KA Core = 6 AI-Search: Search\nCS Core: 1\\. State space representation of a problem a. Specifying states,\ngoals, and operators b. Factoring states into representations (hypothesis\nspaces) c. Problem solving by graph search i. e.g., Graphs as a space, and\ntree traversals as exploration of that space ii. Dynamic construction of the\ngraph (not given upfront) 2\\. Uninformed graph search for problem solving (See\nalso: AL-Foundational) a. Breadth-first search b. Depth-first search i. With\niterative deepening c. Uniform cost search 3\\. Heuristic graph search for\nproblem solving (See also: AL-Strategies) a. Heuristic construction and\nadmissibility b. Hill-climbing c. Local minima and the search landscape i.\nLocal vs global solutions d. Greedy best-first search e. A* search 69 4\\.\nSpace and time complexities of graph search algorithms KA Core: 5\\.\nBidirectional search 6\\. Beam search 7\\. Two-player adversarial games a.\nMinimax search b. Alpha-beta pruning i. Ply cutoff 8\\. Implementation of A*\nsearch 9\\. Constraint satisfaction Non-core: 10\\. Understanding the search\nspace a. Constructing search trees b. Dynamic search spaces c. Combinatorial\nexplosion of search space d. Search space topology (e.g., ridges, saddle\npoints, local minima) 11\\. Local search 12\\. Tabu search 13\\. Variations on A*\n(IDA*, SMA*, RBFS) 14\\. Two-player adversarial games a. The horizon effect b.\nOpening playbooks/endgame solutions c. What it means to \"solve\" a game (e.g.,\ncheckers) 15\\. Implementation of minimax search, beam search 16\\. Expectimax\nsearch (MDP-solving) and chance nodes 17\\. Stochastic search a. Simulated\nannealing b. Genetic algorithms c. Monte-Carlo tree search Illustrative\nLearning Outcomes: 1\\. Design the state space representation for a puzzle\n(e.g., N-queens or 3-jug problem) 2\\. Select and implement an appropriate\nuninformed search algorithm for a problem (e.g., tic-tac-toe), and\ncharacterize its time and space complexities. 3\\. Select and implement an\nappropriate informed search algorithm for a problem after designing a helpful\nheuristic function (e.g., a robot navigating a 2D gridworld). 4\\. Evaluate\nwhether a heuristic for a given problem is admissible/can guarantee an optimal\nsolution. 5\\. Apply minimax search in a two-player adversarial game (e.g.,\nconnect four), using heuristic evaluation at a particular depth to compute the\nscores to back up. [KA Core] 6\\. Design and implement a genetic algorithm\nsolution to a problem. 7\\. Design and implement a simulated annealing schedule\nto avoid local minima in a problem. 70 8\\. Design and implement A*/beam search\nto solve a problem, and compare it against other search algorithms in terms of\nthe solution cost, number of nodes expanded, etc. 9\\. Apply minimax search\nwith alpha-beta pruning to prune search space in a two-player adversarial game\n(e.g., connect four). 10\\. Compare and contrast genetic algorithms with\nclassic search techniques, explaining when it is most appropriate to use a\ngenetic algorithm to learn a model versus other forms of optimization (e.g.,\ngradient descent). 11\\. Compare and contrast various heuristic searches vis-a-\nvis applicability to a given problem. 12\\. Model a logic or Sudoku puzzle as a\nconstraint satisfaction problem, solve it with backtrack search, and determine\nhow much arc consistency can reduce the search space.</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/b57bdcc944fe94b668634b72b05580ba3f2fe43695c2426fa9336bae9214b3f5> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_XMS3IU300" <http://groq.org/b57bdcc944fe94b668634b72b05580ba3f2fe43695c2426fa9336bae9214b3f5> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/has_schema_type> _:N695b301f759644b380a4aec3dc35342a <http://groq.org/b57bdcc944fe94b668634b72b05580ba3f2fe43695c2426fa9336bae9214b3f5> .
<http://example.org/GROK> <http://example.org/param_model> "llama3-8b-8192" <http://groq.org/b57bdcc944fe94b668634b72b05580ba3f2fe43695c2426fa9336bae9214b3f5> .
_:N3c22c19c6c604c1fb4db97a9d1693393 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/4d8c55f9f7c5375ab552518923556f3bdd217165395d451148c11ee35a8b81cf> .
_:N3c22c19c6c604c1fb4db97a9d1693393 <http://schema.org/answer> "1" <http://groq.org/4d8c55f9f7c5375ab552518923556f3bdd217165395d451148c11ee35a8b81cf> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_XMS3IU300" <http://groq.org/4d8c55f9f7c5375ab552518923556f3bdd217165395d451148c11ee35a8b81cf> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/has_schema_type> _:N3c22c19c6c604c1fb4db97a9d1693393 <http://groq.org/4d8c55f9f7c5375ab552518923556f3bdd217165395d451148c11ee35a8b81cf> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Optimisation multi-objectif Objectif: (résultats d'apprentissage)\nproblems (M)\n- To be able to apply the e-constraint algorithm with adaptive step to solve a bi-objective\ncombinatorial problem (A)\n- To be able to apply the weighted sum scalarization to compute a complete set of extreme\nsupported solution of a bi-objective combinatorial optimization problem (A)\n- To be able to define and update the search area of a bi-objective combinatorial optimization\nproblem (Phase 2 of the two phase method) (A)\n- To know the main strategies (variable fixing, ranking, branch and bound) to explore the search\narea (A)\n- To know the notion of bound sets (M)\n- To be able to apply a branch and bound algorithm to solve a bi-objective combinatorial\noptimization problem (A)\n- To be able to apply a dynamic programming algorithm for multi-objective combinatorial\noptimization problem, application to the shortest path problem (A) Course content: * Multi-objective combinatorial optimization - main definitions and properties\n* Multi-objective combinatorial optimization - main solution methods\n* Multi-objective shortest path problem Course name: http://example.org/course/UE_XMS3IU300</page1>\n        <page2>Pages:71-75 HOURS CS Core = 4 KA Core = 6 AI-ML: Machine Learning CS Core: 1\\.\nDefinition and examples of a broad variety of machine learning tasks a.\nSupervised learning i. Classification ii. Regression b. Reinforcement learning\nc. Unsupervised learning i. Clustering 2\\. Fundamental ideas: a. No free lunch\ntheorem: no one learner can solve all problems; representational design\ndecisions have consequences. b. Sources of error and undecidability in machine\nlearning 3\\. A simple statistical-based supervised learning such as linear\nregression or decision trees a. Focus on how they work without going into\nmathematical or optimization details; enough to understand and use existing\nimplementations correctly 4\\. The overfitting problem/controlling solution\ncomplexity (regularization, pruning - intuition only) a. The bias\n(underfitting) - variance (overfitting) tradeoff 5\\. Working with Data a. Data\npreprocessing i. Importance and pitfalls of preprocessing choices b. Handling\nmissing values (imputing, flag-as-missing) i. Implications of imputing vs\nflag-as-missing c. Encoding categorical variables, encoding real-valued data\nd. Normalization/standardization e. Emphasis on real data, not textbook\nexamples 6\\. Representations a. Hypothesis spaces and complexity b. Simple\nbasis feature expansion, such as squaring univariate features c. Learned\nfeature representations 7\\. Machine learning evaluation a. Separation of\ntrain, validation, and test sets b. Performance metrics for classifiers c.\nEstimation of test performance on held-out data d. Tuning the parameters of a\nmachine learning model with a validation set e. Importance of understanding\nwhat a model is doing, where its pitfalls/shortcomings are, and the\nimplications of its decisions 8\\. Basic neural networks a. Fundamentals of\nunderstanding how neural networks work and their training process, without\ndetails of the calculations b. Basic introduction to generative neural\nnetworks (e.g., large language models) 9\\. Ethics for Machine Learning (See\nalso: SEP-Context) a. Focus on real data, real scenarios, and case studies b.\nDataset/algorithmic/evaluation bias and unintended consequences 72 KA Core:\n10\\. Formulation of simple machine learning as an optimization problem, such\nas least squares linear regression or logistic regression a. Objective\nfunction b. Gradient descent c. Regularization to avoid overfitting\n(mathematical formulation) 11\\. Ensembles of models a. Simple weighted\nmajority combination 12\\. Deep learning a. Deep feed-forward networks\n(intuition only, no mathematics) b. Convolutional neural networks (intuition\nonly, no mathematics) c. Visualization of learned feature representations from\ndeep nets d. Other architectures (generative NN, recurrent NN, transformers,\netc.) 13\\. Performance evaluation a. Other metrics for classification (e.g.,\nerror, precision, recall) b. Performance metrics for regressors c. Confusion\nmatrix d. Cross-validation i. Parameter tuning (grid/random search, via cross-\nvalidation) 14\\. Overview of reinforcement learning methods 15\\. Two or more\napplications of machine learning algorithms a. E.g., medicine and health,\neconomics, vision, natural language, robotics, game play 16\\. Ethics for\nMachine Learning a. Continued focus on real data, real scenarios, and case\nstudies (See also: SEP-Context) b. Privacy (See also: SEP-Privacy) c. Fairness\n(See also: SEP-Privacy) d. Intellectual property e. Explainability Non-core:\n17\\. General statistical-based learning, parameter estimation (maximum\nlikelihood) 18\\. Supervised learning a. Decision trees b. Nearest-neighbor\nclassification and regression c. Learning simple neural networks / multi-layer\nperceptrons d. Linear regression e. Logistic regression f. Support vector\nmachines (SVMs) and kernels g. Gaussian Processes 19\\. Overfitting a. The\ncurse of dimensionality b. Regularization (mathematical computations, L2 and\nL1 regularization) 20\\. Experimental design 73 a. Data preparation (e.g.,\nstandardization, representation, one-hot encoding) b. Hypothesis space c.\nBiases (e.g., algorithmic, search) d. Partitioning data: stratification,\ntraining set, validation set, test set e. Parameter tuning (grid/random\nsearch, via cross-validation) f. Performance evaluation i. Cross-validation\nii. Metric: error, precision, recall, confusion matrix iii. Receiver operating\ncharacteristic (ROC) curve and area under ROC curve 21\\. Bayesian learning\n(Cross-Reference AI/Reasoning Under Uncertainty) a. Naive Bayes and its\nrelationship to linear models b. Bayesian networks c. Prior/posterior d.\nGenerative models 22\\. Deep learning a. Deep feed-forward networks b. Neural\ntangent kernel and understanding neural network training c. Convolutional\nneural networks d. Autoencoders e. Recurrent networks f. Representations and\nknowledge transfer g. Adversarial training and generative adversarial networks\nh. Attention mechanisms 23\\. Representations a. Manually crafted\nrepresentations b. Basis expansion c. Learned representations (e.g., deep\nneural networks) 24\\. Unsupervised learning and clustering a. K-means b.\nGaussian mixture models c. Expectation maximization (EM) d. Self-organizing\nmaps 25\\. Graph analysis (e.g., PageRank) 26\\. Semi-supervised learning 27\\.\nGraphical models (See also: AI-Probability) 28\\. Ensembles a. Weighted\nmajority b. Boosting/bagging c. Random forest d. Gated ensemble 29\\. Learning\ntheory a. General overview of learning theory / why learning works b. VC\ndimension c. Generalization bounds 74 30\\. Reinforcement learning a.\nExploration vs exploitation tradeoff b. Markov decision processes c. Value and\npolicy iteration d. Policy gradient methods e. Deep reinforcement learning f.\nLearning from demonstration and inverse RL 31\\. Explainable / interpretable\nmachine learning a. Understanding feature importance (e.g., LIME, Shapley\nvalues) b. Interpretable models and representations 32\\. Recommender systems\n33\\. Hardware for machine learning a. GPUs / TPUs 34\\. Application of machine\nlearning algorithms to: a. Medicine and health b. Economics c. Education d.\nVision e. Natural language f. Robotics g. Game play h. Data mining (Cross-\nreference DM/Data Analytics) 35\\. Ethics for Machine Learning a. Continued\nfocus on real data, real scenarios, and case studies (See also: SEP-Context)\nb. In depth exploration of dataset/algorithmic/evaluation bias, data privacy,\nand fairness (See also: SEP-Privacy, SEP-Context) c. Trust / explainability\nIllustrative Learning Outcomes: 1\\. Describe the differences among the three\nmain styles of learning (supervised, reinforcement, and unsupervised) and\ndetermine which is appropriate to a particular problem domain. 2\\.\nDifferentiate the terms of AI, machine learning, and deep learning. 3\\. Frame\nan application as a classification problem, including the available input\nfeatures and output to be predicted (e.g., identifying alphabetic characters\nfrom pixel grid input). 4\\. Apply two or more simple statistical learning\nalgorithms to a classification task and measure the classifiers' accuracy. 5\\.\nIdentify overfitting in the context of a problem and learning curves and\ndescribe solutions to overfitting. 6\\. Explain how machine learning works as\nan optimization/search process. 7\\. Implement a statistical learning algorithm\nand the corresponding optimization process to train the classifier and obtain\na prediction on new data. 8\\. Describe the neural network training process and\nresulting learned representations. 75 9\\. Explain proper ML evaluation\nprocedures, including the differences between training and testing\nperformance, and what can go wrong with the evaluation process leading to\ninaccurate reporting of ML performance. 10\\. Compare two machine learning\nalgorithms on a dataset, implementing the data preprocessing and evaluation\nmethodology (e.g., metrics and handling of train/test splits) from scratch.\n11\\. Visualize the training progress of a neural network through learning\ncurves in a well-established toolkit (e.g., TensorBoard) and visualize the\nlearned features of the network. 12\\. Compare and contrast several learning\ntechniques (e.g., decision trees, logistic regression, naive Bayes, neural\nnetworks, and belief networks), providing examples of when each strategy is\nsuperior. 13\\. Evaluate the performance of a simple learning system on a real-\nworld dataset. 14\\. Characterize the state of the art in learning theory,\nincluding its achievements and shortcomings. 15\\. Explain the problem of\noverfitting, along with techniques for detecting and managing the problem.\n16\\. Explain the triple tradeoff among the size of a hypothesis space, the\nsize of the training set, and performance accuracy. 17\\. Given a real-world\napplication of machine learning, describe ethical issues regarding the choices\nof data, preprocessing steps, algorithm selection, and\nvisualization/presentation of results</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/4d8c55f9f7c5375ab552518923556f3bdd217165395d451148c11ee35a8b81cf> .
_:N3c22c19c6c604c1fb4db97a9d1693393 <http://schema.org/explain> "The lecture covers a significant portion of the KU, including topics such as multi-objective combinatorial optimization, machine learning, and neural networks." <http://groq.org/4d8c55f9f7c5375ab552518923556f3bdd217165395d451148c11ee35a8b81cf> .
<http://example.org/GROK> <http://example.org/param_model> "llama3-8b-8192" <http://groq.org/4d8c55f9f7c5375ab552518923556f3bdd217165395d451148c11ee35a8b81cf> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Optimisation multi-objectif Objectif: (résultats d'apprentissage)\nproblems (M)\n- To be able to apply the e-constraint algorithm with adaptive step to solve a bi-objective\ncombinatorial problem (A)\n- To be able to apply the weighted sum scalarization to compute a complete set of extreme\nsupported solution of a bi-objective combinatorial optimization problem (A)\n- To be able to define and update the search area of a bi-objective combinatorial optimization\nproblem (Phase 2 of the two phase method) (A)\n- To know the main strategies (variable fixing, ranking, branch and bound) to explore the search\narea (A)\n- To know the notion of bound sets (M)\n- To be able to apply a branch and bound algorithm to solve a bi-objective combinatorial\noptimization problem (A)\n- To be able to apply a dynamic programming algorithm for multi-objective combinatorial\noptimization problem, application to the shortest path problem (A) Course content: * Multi-objective combinatorial optimization - main definitions and properties\n* Multi-objective combinatorial optimization - main solution methods\n* Multi-objective shortest path problem Course name: http://example.org/course/UE_XMS3IU300</page1>\n        <page2>Pages: 142-143 HOURS CS Core = 0 KA Core = 0 FPL-Code: Code Generation Non-\ncore: 1\\. Instruction sets (See also: AR-Assembly) 2\\. Control flow 3\\. Memory\nmanagement (See also: AR-Memory, OS-Memory) 4\\. Procedure calls and method\ndispatching 5\\. Separate compilation; linking 6\\. Instruction selection 7\\.\nInstruction scheduling (e.g., pipelining) 8\\. Register allocation 9\\. Code\noptimization as a form of program analysis (See also: FPL-Analysis) 10\\.\nProgram generation through generative AI Illustrative learning outcomes: Non-\ncore: 1\\. Identify all essential steps for automatically converting source\ncode into assembly or other low-level languages. 2\\. Explain the low-level\ncode necessary for calling functions/methods in modern languages. 3\\. Discuss\nwhy separate compilation requires uniform calling conventions. 4\\. Discuss why\nseparate compilation limits optimization because of unknown effects of calls.\n5\\. Discuss opportunities for optimization introduced by naive translation and\napproaches for achieving. optimization, such as instruction selection,\ninstruction scheduling, register allocation, and peephole optimization.</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/a9bd2d8db14ccabc35a1276538f2e2f620a42a9bf3a84529b4064937a44b5b85> .
_:Nd10b488100584678b96b4c58fc7e7e2a <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/a9bd2d8db14ccabc35a1276538f2e2f620a42a9bf3a84529b4064937a44b5b85> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_XMS3IU300" <http://groq.org/a9bd2d8db14ccabc35a1276538f2e2f620a42a9bf3a84529b4064937a44b5b85> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/has_schema_type> _:Nd10b488100584678b96b4c58fc7e7e2a <http://groq.org/a9bd2d8db14ccabc35a1276538f2e2f620a42a9bf3a84529b4064937a44b5b85> .
_:Nd10b488100584678b96b4c58fc7e7e2a <http://schema.org/answer> "0" <http://groq.org/a9bd2d8db14ccabc35a1276538f2e2f620a42a9bf3a84529b4064937a44b5b85> .
_:Nd10b488100584678b96b4c58fc7e7e2a <http://schema.org/explain> "No overlap in topics: multi-objective optimization vs code generation/compiler internals" <http://groq.org/a9bd2d8db14ccabc35a1276538f2e2f620a42a9bf3a84529b4064937a44b5b85> .
<http://example.org/GROK> <http://example.org/param_model> "qwen-qwq-32b" <http://groq.org/a9bd2d8db14ccabc35a1276538f2e2f620a42a9bf3a84529b4064937a44b5b85> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/prerequisite> "(s)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/evaluation> "Transport et logistique 100%" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/semester> "3" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/location> "UFR des Sciences et des Techniques" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/level> "Master" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/objective> "(résultats d'apprentissage) + Heuristics for vehicle routing problems M\n+ Meta heuristics for vehicle routing problems A\n+ Solving vehicle routing problems with time windows A\n+ Advanced exact methods for facility location and vehicle routing (Branch and cut, Lagrangean\nrelaxation, column generation) I." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/location> "IMT Atlantique" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/level> "Master" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/content> "Alignement de séquences" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/location> "UFR des Sciences et des Techniques" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/language> "Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/language> "Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/obtention> "La répartition entre évaluation écrite/orale/pratique sera donnée à l'occasion du premier cours." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/content> "Méthodes d’enseignement" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/hours> "TOTAL : 24h Répartition : CM : 12h TD : 12h CI : 0h TP : 0h EAD : 0h" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://example.org/course/UE> <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/bibliography> "Inc, 2001\n• Carlos Coello Coello, Gary Lamont, David van Veldhuizen. Evolutionary Algorithms for Solving" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/code> "XMS3IU350" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://www.w3.org/2000/01/rdf-schema#label> "Conférences et projet intégrateur" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/parcours> "Apprentissage et Traitement Automatique de la Langue (ATAL)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/level> "Master" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/semester> "3" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/evaluation> "Algorithmique du génome 100%" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/obtention> "Les dispenses d'assiduité ne sont pas autorisées." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/language> "Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/prerequisite> "(s)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/bibliography> "" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/obtention> "La répartition entre évaluation écrite/orale/pratique sera donnée à l'occasion du premier cours." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/hours> "TOTAL : 24h Répartition : CM : 12h TD : 12h CI : 0h TP : 0h EAD : 0h" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/semester> "4" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/parcours> "M2 CMI-OPTIM" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/responsible> "LEHUEDE Fabien" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/objective> "(résultats d'apprentissage)\nproblems (M)\n- To be able to apply the e-constraint algorithm with adaptive step to solve a bi-objective\ncombinatorial problem (A)\n- To be able to apply the weighted sum scalarization to compute a complete set of extreme\nsupported solution of a bi-objective combinatorial optimization problem (A)\n- To be able to define and update the search area of a bi-objective combinatorial optimization\nproblem (Phase 2 of the two phase method) (A)\n- To know the main strategies (variable fixing, ranking, branch and bound) to explore the search\narea (A)\n- To know the notion of bound sets (M)\n- To be able to apply a branch and bound algorithm to solve a bi-objective combinatorial\noptimization problem (A)\n- To be able to apply a dynamic programming algorithm for multi-objective combinatorial\noptimization problem, application to the shortest path problem (A)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/language> "Mixte" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/bibliography> "" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://example.org/course/UE> <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://www.w3.org/2000/01/rdf-schema#label> "Transport et logistique" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://www.w3.org/2000/01/rdf-schema#label> "Optimisation globale" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/responsible> "ROSSI ANDRE" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/level> "Master" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/semester> "3" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/content> "• Non-dominated Sorting Genetic Algorithm 2 : NSGA-II" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/semester> "3" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/methods> "Langue d’enseignement Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/content> "- Model the core problems and their simple extensions\n- Design some exact and heuritic solving methods for these optimization problems" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/obtention> "La répartition entre évaluation écrite/orale/pratique sera donnée à l'occasion du premier cours." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/location> "UFR des Sciences et des Techniques" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/hours> "TOTAL : 24h Répartition : CM : 12h TD : 12h CI : 0h TP : 0h EAD : 0h" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/prerequisite> "(s)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/prerequisite> "(s)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/evaluation> "Solveurs multi-objectifs 100%" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/hours> "TOTAL : 24h Répartition : CM : 12h TD : 12h CI : 0h TP : 0h EAD : 0h" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/parcours> "M2 Optimisation en Recherche Opérationnelle (ORO)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/methods> "Langue d’enseignement Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/language> "Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/bibliography> "" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/responsible> "JERMANN CHRISTOPHE" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/level> "Master" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/prerequisite> "(s)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/code> "XMS3IU330" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/semester> "3" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/parcours> "M2 Optimisation en Recherche Opérationnelle (ORO)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/parcours> "M2 CMI-OPTIM" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://www.w3.org/2000/01/rdf-schema#label> "Solveurs multi-objectifs" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/content> "Méthodes d’enseignement" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/objective> "(résultats d'apprentissage)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/prerequisite> "(s)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/evaluation> "Optimisation en robotique 100%" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/obtention> "La répartition entre évaluation écrite/orale/pratique sera donnée à l'occasion du premier cours." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://www.w3.org/2000/01/rdf-schema#label> "Optimisation multi-objectif" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/code> "XMS4IU010" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/methods> "Langue d’enseignement Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/code> "XMS3IU310" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/parcours> "M2 Optimisation en Recherche Opérationnelle (ORO)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/content> "• Arithmétique par intervalles\n• Techniques de réduction\n• Techniques de linéarisation\n• Certification des calculs numériques\n• Classe inversée" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://www.w3.org/2000/01/rdf-schema#label> "Contraintes globales" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/parcours> "M2 CMI-OPTIM" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/code> "XMS3IU300" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/obtention> "La répartition entre évaluation écrite/orale/pratique sera donnée à l'occasion du premier cours." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/methods> "Langue d’enseignement Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/location> "UFR des Sciences et des Techniques" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/content> "Méthodes d’enseignement" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/location> "Soutenance à la faculté des sciences et techniques" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/bibliography> "" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://example.org/course/UE> <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/level> "Master" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/hours> "TOTAL : 24h Répartition : CM : 12h TD : 12h CI : 0h TP : 0h EAD : 0h" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://example.org/course/UE> <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/hours> "TOTAL : 24h Répartition : CM : 12h TD : 4h CI : 0h TP : 8h EAD : 0h" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/responsible> "GRANVILLIERS LAURENT" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/methods> "Langue d’enseignement Mixte" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/parcours> "M2 Optimisation en Recherche Opérationnelle (ORO)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/methods> "Langue d’enseignement Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/responsible> "PERRIN MATTHIEU" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/hours> "TOTAL : 24h Répartition : CM : 10h TD : 10h CI : 0h TP : 4h EAD : 0h" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/level> "Master" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/parcours> "M2 CMI-OPTIM" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/obtention> "La répartition entre évaluation écrite/orale/pratique sera donnée à l'occasion du premier cours." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/location> "UFR des sciences et des techniques" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/parcours> "M2 Optimisation en Recherche Opérationnelle (ORO)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/bibliography> "Dernière modification par VIRGINIE BLOT, le 2024-08-22 16:10:56" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://www.w3.org/2000/01/rdf-schema#label> "Optimisation en robotique" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/parcours> "M2 CMI-OPTIM" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/responsible> "TONNEAU QUENTIN" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/location> "IMT Atlantique" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/objective> "(résultats d'apprentissage)\nmétaheuristique multiobjectif.\n• Appréhender et conduire des cas d'étude." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/obtention> "La répartition entre évaluation écrite/orale/pratique sera donnée à l'occasion du premier cours." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/language> "Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/language> "Anglais\n• Ralph E. Steuer. Multiple Criteria Optimization : Theory, Computation, and Application. Wiley" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/objective> "(résultats d'apprentissage) - Synthétiser le travail effectué dans un compte rendu de manière concise, correcte, et complète.\n- Préparer des supports puis présenter oralement le travail effectué de manière dynamique et\nconvaincante." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/evaluation> "Ordonnancement et planification 100%" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/location> "UFR des Sciences et des Techniques" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/parcours> "M2 CMI-OPTIM" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/code> "XMS3WU380" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/hours> "TOTAL : 24h Répartition : CM : 10.67h TD : 13.33h CI : 0h TP : 0h EAD : 0h" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/level> "Master" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/hours> "TOTAL : 24h Répartition : CM : 12h TD : 12h CI : 0h TP : 0h EAD : 0h" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/level> "Master" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://example.org/course/UE> <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/objective> "(résultats d'apprentissage)\noptimization problems (Analysis) ;\n* model a variety of robotic problems as numerical constrained optimization problems and solve\nthem using an appropriate method (Synthesis) ;" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/methods> "Langue d’enseignement Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/hours> "TOTAL : 0h Répartition : CM : 0h TD : 0h CI : 0h TP : 0h EAD : 0h" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/obtention> "La répartition entre évaluation écrite/orale/pratique sera donnée à l'occasion du premier cours." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/code> "XMS3IU340" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://example.org/course/UE> <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/bibliography> "" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/objective> "(résultats d'apprentissage)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/parcours> "M2 Optimisation en Recherche Opérationnelle (ORO)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/evaluation> "Stage 100%" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/evaluation> "Optimisation multi-objectif 100%" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/parcours> "M2 CMI-OPTIM" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/bibliography> "Cf. les 60 références du polycopié de cours" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/methods> "• Projet de développement (logiciel, cas d'étude)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/semester> "3" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/language> "Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/objective> "(résultats d'apprentissage) form of a mixed integer linear program (A)\n-To be able to detect symmetry in a problem or model, and to know how to cope with this\nphenomenon (A-M)\n- To apprehend Lagragian relaxation with effective implementation (M)\n- To discover the implementation of a column generation algorithm (I)\n-To understand the theory of Dantzig-Wolfe decomposition and to devise a column generation\napproach for various combinatorial optimization problems (M)\n-To know Branch-and-Price and the different ways to make it efficient (A)\n-To know the Benders decomposition and its relation with Dantzig-Wolfe decomposition (A)\n-To know how matheuristics work for both exact methods using approximate methods, and\nheuristics taking advantage of exact approaches (M)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/level> "Master" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/parcours> "M2 CMI-OPTIM" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/prerequisite> "(s)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/methods> "Langue d’enseignement Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/parcours> "M2 Optimisation en Recherche Opérationnelle (ORO)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://www.w3.org/2000/01/rdf-schema#label> "Algorithmique du génome" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/prerequisite> "(s)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/responsible> "PRZYBYLSKI ANTHONY" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/objective> "(résultats d'apprentissage) • Connaître les stratégies de recherche usuelles\n• Mettre en oeuvre un solveur de problèmes d'optimisation non linéaires" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/bibliography> "" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/evaluation> "Contraintes globales 100%" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/content> "demonstrates how the optimization techniques learnt throughout the Master allow solving them." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://example.org/course/UE> <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/hours> "TOTAL : 24h Répartition : CM : 12h TD : 12h CI : 0h TP : 0h EAD : 0h" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://www.w3.org/2000/01/rdf-schema#label> "Programmation mathématique" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/code> "XMS3IU320" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/code> "XMS3IU360" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/parcours> "M2 CMI-OPTIM" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/methods> "• Cours ex-cathedra, exercices, mise en situation" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://example.org/course/UE> <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/parcours> "M2 CMI-OPTIM" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/code> "XMS3WU370" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/obtention> "La répartition entre évaluation écrite/orale/pratique sera donnée à l'occasion du premier cours." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/evaluation> "Optimisation globale 100%" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/language> "Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/semester> "3" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://www.w3.org/2000/01/rdf-schema#label> "Stage" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/responsible> "GANDIBLEUX XAVIER" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS4IU010> <http://example.org/course/parcours> "M2 CMI-OPTIM" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://www.w3.org/2000/01/rdf-schema#label> "Ordonnancement et planification" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/methods> "Présentiel." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/prerequisite> "(s)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://example.org/course/parcours> "M2 Optimisation en Recherche Opérationnelle (ORO)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/objective> "(résultats d'apprentissage) Comprendre, reproduire et adapter les algorithmes de recherche de motifs" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/content> "Méthodes d’enseignement" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/responsible> "Bellenguez Odile" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/responsible> "BELDICEANU Nicolas" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/evaluation> "Conférences et projet intégrateur 100%" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/prerequisite> "(s)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/responsible> "RUSU Irena" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://example.org/course/UE> <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/bibliography> "" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU370> <http://example.org/course/objective> "(résultats d'apprentissage)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/evaluation> "Programmation mathématique 100%" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://example.org/course/UE> <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU320> <http://example.org/course/location> "UFR des Sciences et des Techniques" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/obtention> "La répartition entre évaluation écrite/orale/pratique sera donnée à l'occasion du premier cours." <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/location> "IMT Atlantique" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/semester> "3" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/level> "Master" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/semester> "3" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU350> <http://example.org/course/language> "Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/parcours> "M2 Optimisation en Recherche Opérationnelle (ORO)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/parcours> "M2 Optimisation en Recherche Opérationnelle (ORO)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU330> <http://example.org/course/bibliography> "" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/code> "XMS3WU390" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU340> <http://example.org/course/prerequisite> "(s)" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/course/content> "* Multi-objective combinatorial optimization - main definitions and properties\n* Multi-objective combinatorial optimization - main solution methods\n* Multi-objective shortest path problem" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU390> <http://example.org/course/language> "Anglais" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU310> <http://example.org/course/content> "Méthodes d’enseignement" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3WU380> <http://example.org/course/semester> "3" <urn:x-rdflib:default> .
<http://example.org/course/UE_XMS3IU360> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://example.org/course/UE> <urn:x-rdflib:default> .
_:N4400b548bc664a05abd9250c8b8facf9 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/f4951b9c6291f6db95eb2265e847dfa091e5c03adbc857e9e69116b19247c20b> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/has_schema_type> _:N4400b548bc664a05abd9250c8b8facf9 <http://groq.org/f4951b9c6291f6db95eb2265e847dfa091e5c03adbc857e9e69116b19247c20b> .
_:N4400b548bc664a05abd9250c8b8facf9 <http://schema.org/answer> "0" <http://groq.org/f4951b9c6291f6db95eb2265e847dfa091e5c03adbc857e9e69116b19247c20b> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_XMS3IU300" <http://groq.org/f4951b9c6291f6db95eb2265e847dfa091e5c03adbc857e9e69116b19247c20b> .
<http://example.org/GROK> <http://example.org/param_model> "deepseek-r1-distill-llama-70b" <http://groq.org/f4951b9c6291f6db95eb2265e847dfa091e5c03adbc857e9e69116b19247c20b> .
_:N4400b548bc664a05abd9250c8b8facf9 <http://schema.org/explain> "The lecture focuses on multi-objective optimization methods, while the KU covers a broader range of search algorithms and techniques." <http://groq.org/f4951b9c6291f6db95eb2265e847dfa091e5c03adbc857e9e69116b19247c20b> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Optimisation multi-objectif Objectif: (résultats d'apprentissage)\nproblems (M)\n- To be able to apply the e-constraint algorithm with adaptive step to solve a bi-objective\ncombinatorial problem (A)\n- To be able to apply the weighted sum scalarization to compute a complete set of extreme\nsupported solution of a bi-objective combinatorial optimization problem (A)\n- To be able to define and update the search area of a bi-objective combinatorial optimization\nproblem (Phase 2 of the two phase method) (A)\n- To know the main strategies (variable fixing, ranking, branch and bound) to explore the search\narea (A)\n- To know the notion of bound sets (M)\n- To be able to apply a branch and bound algorithm to solve a bi-objective combinatorial\noptimization problem (A)\n- To be able to apply a dynamic programming algorithm for multi-objective combinatorial\noptimization problem, application to the shortest path problem (A) Course content: * Multi-objective combinatorial optimization - main definitions and properties\n* Multi-objective combinatorial optimization - main solution methods\n* Multi-objective shortest path problem Course name: http://example.org/course/UE_XMS3IU300</page1>\n        <page2>AI-Search: Search Pages: 68-70 HOURS CS Core = 5 KA Core = 6 AI-Search: Search\nCS Core: 1\\. State space representation of a problem a. Specifying states,\ngoals, and operators b. Factoring states into representations (hypothesis\nspaces) c. Problem solving by graph search i. e.g., Graphs as a space, and\ntree traversals as exploration of that space ii. Dynamic construction of the\ngraph (not given upfront) 2\\. Uninformed graph search for problem solving (See\nalso: AL-Foundational) a. Breadth-first search b. Depth-first search i. With\niterative deepening c. Uniform cost search 3\\. Heuristic graph search for\nproblem solving (See also: AL-Strategies) a. Heuristic construction and\nadmissibility b. Hill-climbing c. Local minima and the search landscape i.\nLocal vs global solutions d. Greedy best-first search e. A* search 69 4\\.\nSpace and time complexities of graph search algorithms KA Core: 5\\.\nBidirectional search 6\\. Beam search 7\\. Two-player adversarial games a.\nMinimax search b. Alpha-beta pruning i. Ply cutoff 8\\. Implementation of A*\nsearch 9\\. Constraint satisfaction Non-core: 10\\. Understanding the search\nspace a. Constructing search trees b. Dynamic search spaces c. Combinatorial\nexplosion of search space d. Search space topology (e.g., ridges, saddle\npoints, local minima) 11\\. Local search 12\\. Tabu search 13\\. Variations on A*\n(IDA*, SMA*, RBFS) 14\\. Two-player adversarial games a. The horizon effect b.\nOpening playbooks/endgame solutions c. What it means to \"solve\" a game (e.g.,\ncheckers) 15\\. Implementation of minimax search, beam search 16\\. Expectimax\nsearch (MDP-solving) and chance nodes 17\\. Stochastic search a. Simulated\nannealing b. Genetic algorithms c. Monte-Carlo tree search Illustrative\nLearning Outcomes: 1\\. Design the state space representation for a puzzle\n(e.g., N-queens or 3-jug problem) 2\\. Select and implement an appropriate\nuninformed search algorithm for a problem (e.g., tic-tac-toe), and\ncharacterize its time and space complexities. 3\\. Select and implement an\nappropriate informed search algorithm for a problem after designing a helpful\nheuristic function (e.g., a robot navigating a 2D gridworld). 4\\. Evaluate\nwhether a heuristic for a given problem is admissible/can guarantee an optimal\nsolution. 5\\. Apply minimax search in a two-player adversarial game (e.g.,\nconnect four), using heuristic evaluation at a particular depth to compute the\nscores to back up. [KA Core] 6\\. Design and implement a genetic algorithm\nsolution to a problem. 7\\. Design and implement a simulated annealing schedule\nto avoid local minima in a problem. 70 8\\. Design and implement A*/beam search\nto solve a problem, and compare it against other search algorithms in terms of\nthe solution cost, number of nodes expanded, etc. 9\\. Apply minimax search\nwith alpha-beta pruning to prune search space in a two-player adversarial game\n(e.g., connect four). 10\\. Compare and contrast genetic algorithms with\nclassic search techniques, explaining when it is most appropriate to use a\ngenetic algorithm to learn a model versus other forms of optimization (e.g.,\ngradient descent). 11\\. Compare and contrast various heuristic searches vis-a-\nvis applicability to a given problem. 12\\. Model a logic or Sudoku puzzle as a\nconstraint satisfaction problem, solve it with backtrack search, and determine\nhow much arc consistency can reduce the search space.</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/f4951b9c6291f6db95eb2265e847dfa091e5c03adbc857e9e69116b19247c20b> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/has_schema_type> _:Nc492a7b6ad1f40e7914f6bd6650f02df <http://groq.org/fac21914445adade47aac68e08e08609af75d576f7ce65c47b9a4dd7d6a3ba1e> .
_:Nc492a7b6ad1f40e7914f6bd6650f02df <http://schema.org/explain> "Lecture focuses on multi-objective optimization methods (e.g., e-constraint, weighted sum), while KU covers broader search algorithms (A*, minimax, genetic algorithms) not substantially addressed." <http://groq.org/fac21914445adade47aac68e08e08609af75d576f7ce65c47b9a4dd7d6a3ba1e> .
_:Nc492a7b6ad1f40e7914f6bd6650f02df <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/fac21914445adade47aac68e08e08609af75d576f7ce65c47b9a4dd7d6a3ba1e> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_XMS3IU300" <http://groq.org/fac21914445adade47aac68e08e08609af75d576f7ce65c47b9a4dd7d6a3ba1e> .
_:Nc492a7b6ad1f40e7914f6bd6650f02df <http://schema.org/answer> "0" <http://groq.org/fac21914445adade47aac68e08e08609af75d576f7ce65c47b9a4dd7d6a3ba1e> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Optimisation multi-objectif Objectif: (résultats d'apprentissage)\nproblems (M)\n- To be able to apply the e-constraint algorithm with adaptive step to solve a bi-objective\ncombinatorial problem (A)\n- To be able to apply the weighted sum scalarization to compute a complete set of extreme\nsupported solution of a bi-objective combinatorial optimization problem (A)\n- To be able to define and update the search area of a bi-objective combinatorial optimization\nproblem (Phase 2 of the two phase method) (A)\n- To know the main strategies (variable fixing, ranking, branch and bound) to explore the search\narea (A)\n- To know the notion of bound sets (M)\n- To be able to apply a branch and bound algorithm to solve a bi-objective combinatorial\noptimization problem (A)\n- To be able to apply a dynamic programming algorithm for multi-objective combinatorial\noptimization problem, application to the shortest path problem (A) Course content: * Multi-objective combinatorial optimization - main definitions and properties\n* Multi-objective combinatorial optimization - main solution methods\n* Multi-objective shortest path problem Course name: http://example.org/course/UE_XMS3IU300</page1>\n        <page2>AI-Search: Search Pages: 68-70 HOURS CS Core = 5 KA Core = 6 AI-Search: Search\nCS Core: 1\\. State space representation of a problem a. Specifying states,\ngoals, and operators b. Factoring states into representations (hypothesis\nspaces) c. Problem solving by graph search i. e.g., Graphs as a space, and\ntree traversals as exploration of that space ii. Dynamic construction of the\ngraph (not given upfront) 2\\. Uninformed graph search for problem solving (See\nalso: AL-Foundational) a. Breadth-first search b. Depth-first search i. With\niterative deepening c. Uniform cost search 3\\. Heuristic graph search for\nproblem solving (See also: AL-Strategies) a. Heuristic construction and\nadmissibility b. Hill-climbing c. Local minima and the search landscape i.\nLocal vs global solutions d. Greedy best-first search e. A* search 69 4\\.\nSpace and time complexities of graph search algorithms KA Core: 5\\.\nBidirectional search 6\\. Beam search 7\\. Two-player adversarial games a.\nMinimax search b. Alpha-beta pruning i. Ply cutoff 8\\. Implementation of A*\nsearch 9\\. Constraint satisfaction Non-core: 10\\. Understanding the search\nspace a. Constructing search trees b. Dynamic search spaces c. Combinatorial\nexplosion of search space d. Search space topology (e.g., ridges, saddle\npoints, local minima) 11\\. Local search 12\\. Tabu search 13\\. Variations on A*\n(IDA*, SMA*, RBFS) 14\\. Two-player adversarial games a. The horizon effect b.\nOpening playbooks/endgame solutions c. What it means to \"solve\" a game (e.g.,\ncheckers) 15\\. Implementation of minimax search, beam search 16\\. Expectimax\nsearch (MDP-solving) and chance nodes 17\\. Stochastic search a. Simulated\nannealing b. Genetic algorithms c. Monte-Carlo tree search Illustrative\nLearning Outcomes: 1\\. Design the state space representation for a puzzle\n(e.g., N-queens or 3-jug problem) 2\\. Select and implement an appropriate\nuninformed search algorithm for a problem (e.g., tic-tac-toe), and\ncharacterize its time and space complexities. 3\\. Select and implement an\nappropriate informed search algorithm for a problem after designing a helpful\nheuristic function (e.g., a robot navigating a 2D gridworld). 4\\. Evaluate\nwhether a heuristic for a given problem is admissible/can guarantee an optimal\nsolution. 5\\. Apply minimax search in a two-player adversarial game (e.g.,\nconnect four), using heuristic evaluation at a particular depth to compute the\nscores to back up. [KA Core] 6\\. Design and implement a genetic algorithm\nsolution to a problem. 7\\. Design and implement a simulated annealing schedule\nto avoid local minima in a problem. 70 8\\. Design and implement A*/beam search\nto solve a problem, and compare it against other search algorithms in terms of\nthe solution cost, number of nodes expanded, etc. 9\\. Apply minimax search\nwith alpha-beta pruning to prune search space in a two-player adversarial game\n(e.g., connect four). 10\\. Compare and contrast genetic algorithms with\nclassic search techniques, explaining when it is most appropriate to use a\ngenetic algorithm to learn a model versus other forms of optimization (e.g.,\ngradient descent). 11\\. Compare and contrast various heuristic searches vis-a-\nvis applicability to a given problem. 12\\. Model a logic or Sudoku puzzle as a\nconstraint satisfaction problem, solve it with backtrack search, and determine\nhow much arc consistency can reduce the search space.</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/fac21914445adade47aac68e08e08609af75d576f7ce65c47b9a4dd7d6a3ba1e> .
<http://example.org/GROK> <http://example.org/param_model> "qwen-qwq-32b" <http://groq.org/fac21914445adade47aac68e08e08609af75d576f7ce65c47b9a4dd7d6a3ba1e> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Optimisation multi-objectif Objectif: (résultats d'apprentissage)\nproblems (M)\n- To be able to apply the e-constraint algorithm with adaptive step to solve a bi-objective\ncombinatorial problem (A)\n- To be able to apply the weighted sum scalarization to compute a complete set of extreme\nsupported solution of a bi-objective combinatorial optimization problem (A)\n- To be able to define and update the search area of a bi-objective combinatorial optimization\nproblem (Phase 2 of the two phase method) (A)\n- To know the main strategies (variable fixing, ranking, branch and bound) to explore the search\narea (A)\n- To know the notion of bound sets (M)\n- To be able to apply a branch and bound algorithm to solve a bi-objective combinatorial\noptimization problem (A)\n- To be able to apply a dynamic programming algorithm for multi-objective combinatorial\noptimization problem, application to the shortest path problem (A) Course content: * Multi-objective combinatorial optimization - main definitions and properties\n* Multi-objective combinatorial optimization - main solution methods\n* Multi-objective shortest path problem Course name: http://example.org/course/UE_XMS3IU300</page1>\n        <page2>Pages: 142-143 HOURS CS Core = 0 KA Core = 0 FPL-Code: Code Generation Non-\ncore: 1\\. Instruction sets (See also: AR-Assembly) 2\\. Control flow 3\\. Memory\nmanagement (See also: AR-Memory, OS-Memory) 4\\. Procedure calls and method\ndispatching 5\\. Separate compilation; linking 6\\. Instruction selection 7\\.\nInstruction scheduling (e.g., pipelining) 8\\. Register allocation 9\\. Code\noptimization as a form of program analysis (See also: FPL-Analysis) 10\\.\nProgram generation through generative AI Illustrative learning outcomes: Non-\ncore: 1\\. Identify all essential steps for automatically converting source\ncode into assembly or other low-level languages. 2\\. Explain the low-level\ncode necessary for calling functions/methods in modern languages. 3\\. Discuss\nwhy separate compilation requires uniform calling conventions. 4\\. Discuss why\nseparate compilation limits optimization because of unknown effects of calls.\n5\\. Discuss opportunities for optimization introduced by naive translation and\napproaches for achieving. optimization, such as instruction selection,\ninstruction scheduling, register allocation, and peephole optimization.</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/54250d70f37d45a7f24eeb1aa20e7f8795bb112792f1e601caffc394745ef99a> .
_:Nebfc1c7d0aac46b49cb3088acb9fcb16 <http://schema.org/answer> "0" <http://groq.org/54250d70f37d45a7f24eeb1aa20e7f8795bb112792f1e601caffc394745ef99a> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_XMS3IU300" <http://groq.org/54250d70f37d45a7f24eeb1aa20e7f8795bb112792f1e601caffc394745ef99a> .
_:Nebfc1c7d0aac46b49cb3088acb9fcb16 <http://schema.org/explain> "The lecture focuses on multi-objective optimization algorithms, while the KU covers code generation and optimization, with no overlap in content." <http://groq.org/54250d70f37d45a7f24eeb1aa20e7f8795bb112792f1e601caffc394745ef99a> .
<http://example.org/GROK> <http://example.org/param_model> "deepseek-r1-distill-llama-70b" <http://groq.org/54250d70f37d45a7f24eeb1aa20e7f8795bb112792f1e601caffc394745ef99a> .
_:Nebfc1c7d0aac46b49cb3088acb9fcb16 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/54250d70f37d45a7f24eeb1aa20e7f8795bb112792f1e601caffc394745ef99a> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/has_schema_type> _:Nebfc1c7d0aac46b49cb3088acb9fcb16 <http://groq.org/54250d70f37d45a7f24eeb1aa20e7f8795bb112792f1e601caffc394745ef99a> .
_:Ne475b90835f34a28b5e27e81d3187c27 <http://schema.org/explain> "Lecture focuses on multi-objective optimization algorithms, while KU covers machine learning fundamentals, ethics, and evaluation—distinct domains." <http://groq.org/f7af3a98d212bea7c3a11f494d8ead02cf8dd356e08f47d429f3a0980b356f16> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/has_schema_type> _:Ne475b90835f34a28b5e27e81d3187c27 <http://groq.org/f7af3a98d212bea7c3a11f494d8ead02cf8dd356e08f47d429f3a0980b356f16> .
_:Ne475b90835f34a28b5e27e81d3187c27 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/f7af3a98d212bea7c3a11f494d8ead02cf8dd356e08f47d429f3a0980b356f16> .
_:Ne475b90835f34a28b5e27e81d3187c27 <http://schema.org/answer> "0" <http://groq.org/f7af3a98d212bea7c3a11f494d8ead02cf8dd356e08f47d429f3a0980b356f16> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_XMS3IU300" <http://groq.org/f7af3a98d212bea7c3a11f494d8ead02cf8dd356e08f47d429f3a0980b356f16> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Optimisation multi-objectif Objectif: (résultats d'apprentissage)\nproblems (M)\n- To be able to apply the e-constraint algorithm with adaptive step to solve a bi-objective\ncombinatorial problem (A)\n- To be able to apply the weighted sum scalarization to compute a complete set of extreme\nsupported solution of a bi-objective combinatorial optimization problem (A)\n- To be able to define and update the search area of a bi-objective combinatorial optimization\nproblem (Phase 2 of the two phase method) (A)\n- To know the main strategies (variable fixing, ranking, branch and bound) to explore the search\narea (A)\n- To know the notion of bound sets (M)\n- To be able to apply a branch and bound algorithm to solve a bi-objective combinatorial\noptimization problem (A)\n- To be able to apply a dynamic programming algorithm for multi-objective combinatorial\noptimization problem, application to the shortest path problem (A) Course content: * Multi-objective combinatorial optimization - main definitions and properties\n* Multi-objective combinatorial optimization - main solution methods\n* Multi-objective shortest path problem Course name: http://example.org/course/UE_XMS3IU300</page1>\n        <page2>Pages:71-75 HOURS CS Core = 4 KA Core = 6 AI-ML: Machine Learning CS Core: 1\\.\nDefinition and examples of a broad variety of machine learning tasks a.\nSupervised learning i. Classification ii. Regression b. Reinforcement learning\nc. Unsupervised learning i. Clustering 2\\. Fundamental ideas: a. No free lunch\ntheorem: no one learner can solve all problems; representational design\ndecisions have consequences. b. Sources of error and undecidability in machine\nlearning 3\\. A simple statistical-based supervised learning such as linear\nregression or decision trees a. Focus on how they work without going into\nmathematical or optimization details; enough to understand and use existing\nimplementations correctly 4\\. The overfitting problem/controlling solution\ncomplexity (regularization, pruning - intuition only) a. The bias\n(underfitting) - variance (overfitting) tradeoff 5\\. Working with Data a. Data\npreprocessing i. Importance and pitfalls of preprocessing choices b. Handling\nmissing values (imputing, flag-as-missing) i. Implications of imputing vs\nflag-as-missing c. Encoding categorical variables, encoding real-valued data\nd. Normalization/standardization e. Emphasis on real data, not textbook\nexamples 6\\. Representations a. Hypothesis spaces and complexity b. Simple\nbasis feature expansion, such as squaring univariate features c. Learned\nfeature representations 7\\. Machine learning evaluation a. Separation of\ntrain, validation, and test sets b. Performance metrics for classifiers c.\nEstimation of test performance on held-out data d. Tuning the parameters of a\nmachine learning model with a validation set e. Importance of understanding\nwhat a model is doing, where its pitfalls/shortcomings are, and the\nimplications of its decisions 8\\. Basic neural networks a. Fundamentals of\nunderstanding how neural networks work and their training process, without\ndetails of the calculations b. Basic introduction to generative neural\nnetworks (e.g., large language models) 9\\. Ethics for Machine Learning (See\nalso: SEP-Context) a. Focus on real data, real scenarios, and case studies b.\nDataset/algorithmic/evaluation bias and unintended consequences 72 KA Core:\n10\\. Formulation of simple machine learning as an optimization problem, such\nas least squares linear regression or logistic regression a. Objective\nfunction b. Gradient descent c. Regularization to avoid overfitting\n(mathematical formulation) 11\\. Ensembles of models a. Simple weighted\nmajority combination 12\\. Deep learning a. Deep feed-forward networks\n(intuition only, no mathematics) b. Convolutional neural networks (intuition\nonly, no mathematics) c. Visualization of learned feature representations from\ndeep nets d. Other architectures (generative NN, recurrent NN, transformers,\netc.) 13\\. Performance evaluation a. Other metrics for classification (e.g.,\nerror, precision, recall) b. Performance metrics for regressors c. Confusion\nmatrix d. Cross-validation i. Parameter tuning (grid/random search, via cross-\nvalidation) 14\\. Overview of reinforcement learning methods 15\\. Two or more\napplications of machine learning algorithms a. E.g., medicine and health,\neconomics, vision, natural language, robotics, game play 16\\. Ethics for\nMachine Learning a. Continued focus on real data, real scenarios, and case\nstudies (See also: SEP-Context) b. Privacy (See also: SEP-Privacy) c. Fairness\n(See also: SEP-Privacy) d. Intellectual property e. Explainability Non-core:\n17\\. General statistical-based learning, parameter estimation (maximum\nlikelihood) 18\\. Supervised learning a. Decision trees b. Nearest-neighbor\nclassification and regression c. Learning simple neural networks / multi-layer\nperceptrons d. Linear regression e. Logistic regression f. Support vector\nmachines (SVMs) and kernels g. Gaussian Processes 19\\. Overfitting a. The\ncurse of dimensionality b. Regularization (mathematical computations, L2 and\nL1 regularization) 20\\. Experimental design 73 a. Data preparation (e.g.,\nstandardization, representation, one-hot encoding) b. Hypothesis space c.\nBiases (e.g., algorithmic, search) d. Partitioning data: stratification,\ntraining set, validation set, test set e. Parameter tuning (grid/random\nsearch, via cross-validation) f. Performance evaluation i. Cross-validation\nii. Metric: error, precision, recall, confusion matrix iii. Receiver operating\ncharacteristic (ROC) curve and area under ROC curve 21\\. Bayesian learning\n(Cross-Reference AI/Reasoning Under Uncertainty) a. Naive Bayes and its\nrelationship to linear models b. Bayesian networks c. Prior/posterior d.\nGenerative models 22\\. Deep learning a. Deep feed-forward networks b. Neural\ntangent kernel and understanding neural network training c. Convolutional\nneural networks d. Autoencoders e. Recurrent networks f. Representations and\nknowledge transfer g. Adversarial training and generative adversarial networks\nh. Attention mechanisms 23\\. Representations a. Manually crafted\nrepresentations b. Basis expansion c. Learned representations (e.g., deep\nneural networks) 24\\. Unsupervised learning and clustering a. K-means b.\nGaussian mixture models c. Expectation maximization (EM) d. Self-organizing\nmaps 25\\. Graph analysis (e.g., PageRank) 26\\. Semi-supervised learning 27\\.\nGraphical models (See also: AI-Probability) 28\\. Ensembles a. Weighted\nmajority b. Boosting/bagging c. Random forest d. Gated ensemble 29\\. Learning\ntheory a. General overview of learning theory / why learning works b. VC\ndimension c. Generalization bounds 74 30\\. Reinforcement learning a.\nExploration vs exploitation tradeoff b. Markov decision processes c. Value and\npolicy iteration d. Policy gradient methods e. Deep reinforcement learning f.\nLearning from demonstration and inverse RL 31\\. Explainable / interpretable\nmachine learning a. Understanding feature importance (e.g., LIME, Shapley\nvalues) b. Interpretable models and representations 32\\. Recommender systems\n33\\. Hardware for machine learning a. GPUs / TPUs 34\\. Application of machine\nlearning algorithms to: a. Medicine and health b. Economics c. Education d.\nVision e. Natural language f. Robotics g. Game play h. Data mining (Cross-\nreference DM/Data Analytics) 35\\. Ethics for Machine Learning a. Continued\nfocus on real data, real scenarios, and case studies (See also: SEP-Context)\nb. In depth exploration of dataset/algorithmic/evaluation bias, data privacy,\nand fairness (See also: SEP-Privacy, SEP-Context) c. Trust / explainability\nIllustrative Learning Outcomes: 1\\. Describe the differences among the three\nmain styles of learning (supervised, reinforcement, and unsupervised) and\ndetermine which is appropriate to a particular problem domain. 2\\.\nDifferentiate the terms of AI, machine learning, and deep learning. 3\\. Frame\nan application as a classification problem, including the available input\nfeatures and output to be predicted (e.g., identifying alphabetic characters\nfrom pixel grid input). 4\\. Apply two or more simple statistical learning\nalgorithms to a classification task and measure the classifiers' accuracy. 5\\.\nIdentify overfitting in the context of a problem and learning curves and\ndescribe solutions to overfitting. 6\\. Explain how machine learning works as\nan optimization/search process. 7\\. Implement a statistical learning algorithm\nand the corresponding optimization process to train the classifier and obtain\na prediction on new data. 8\\. Describe the neural network training process and\nresulting learned representations. 75 9\\. Explain proper ML evaluation\nprocedures, including the differences between training and testing\nperformance, and what can go wrong with the evaluation process leading to\ninaccurate reporting of ML performance. 10\\. Compare two machine learning\nalgorithms on a dataset, implementing the data preprocessing and evaluation\nmethodology (e.g., metrics and handling of train/test splits) from scratch.\n11\\. Visualize the training progress of a neural network through learning\ncurves in a well-established toolkit (e.g., TensorBoard) and visualize the\nlearned features of the network. 12\\. Compare and contrast several learning\ntechniques (e.g., decision trees, logistic regression, naive Bayes, neural\nnetworks, and belief networks), providing examples of when each strategy is\nsuperior. 13\\. Evaluate the performance of a simple learning system on a real-\nworld dataset. 14\\. Characterize the state of the art in learning theory,\nincluding its achievements and shortcomings. 15\\. Explain the problem of\noverfitting, along with techniques for detecting and managing the problem.\n16\\. Explain the triple tradeoff among the size of a hypothesis space, the\nsize of the training set, and performance accuracy. 17\\. Given a real-world\napplication of machine learning, describe ethical issues regarding the choices\nof data, preprocessing steps, algorithm selection, and\nvisualization/presentation of results</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/f7af3a98d212bea7c3a11f494d8ead02cf8dd356e08f47d429f3a0980b356f16> .
<http://example.org/GROK> <http://example.org/param_model> "qwen-qwq-32b" <http://groq.org/f7af3a98d212bea7c3a11f494d8ead02cf8dd356e08f47d429f3a0980b356f16> .
_:N6834e785252c4a31b1e479e199bfdc25 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/a74ee824765fe42211653006a319ada67bc317f397c20f93ac36db10cc856499> .
_:N6834e785252c4a31b1e479e199bfdc25 <http://schema.org/explain> "The lecture focuses on multi-objective optimization, which is a specific area within machine learning. The KU covers a broader range of machine learning topics, including supervised, unsupervised, and reinforcement learning, as well as ethics and applications. The lecture does not substantially cover the full scope of the KU." <http://groq.org/a74ee824765fe42211653006a319ada67bc317f397c20f93ac36db10cc856499> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_XMS3IU300" <http://groq.org/a74ee824765fe42211653006a319ada67bc317f397c20f93ac36db10cc856499> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/has_schema_type> _:N6834e785252c4a31b1e479e199bfdc25 <http://groq.org/a74ee824765fe42211653006a319ada67bc317f397c20f93ac36db10cc856499> .
<http://example.org/GROK> <http://example.org/param_model> "deepseek-r1-distill-llama-70b" <http://groq.org/a74ee824765fe42211653006a319ada67bc317f397c20f93ac36db10cc856499> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Optimisation multi-objectif Objectif: (résultats d'apprentissage)\nproblems (M)\n- To be able to apply the e-constraint algorithm with adaptive step to solve a bi-objective\ncombinatorial problem (A)\n- To be able to apply the weighted sum scalarization to compute a complete set of extreme\nsupported solution of a bi-objective combinatorial optimization problem (A)\n- To be able to define and update the search area of a bi-objective combinatorial optimization\nproblem (Phase 2 of the two phase method) (A)\n- To know the main strategies (variable fixing, ranking, branch and bound) to explore the search\narea (A)\n- To know the notion of bound sets (M)\n- To be able to apply a branch and bound algorithm to solve a bi-objective combinatorial\noptimization problem (A)\n- To be able to apply a dynamic programming algorithm for multi-objective combinatorial\noptimization problem, application to the shortest path problem (A) Course content: * Multi-objective combinatorial optimization - main definitions and properties\n* Multi-objective combinatorial optimization - main solution methods\n* Multi-objective shortest path problem Course name: http://example.org/course/UE_XMS3IU300</page1>\n        <page2>Pages:71-75 HOURS CS Core = 4 KA Core = 6 AI-ML: Machine Learning CS Core: 1\\.\nDefinition and examples of a broad variety of machine learning tasks a.\nSupervised learning i. Classification ii. Regression b. Reinforcement learning\nc. Unsupervised learning i. Clustering 2\\. Fundamental ideas: a. No free lunch\ntheorem: no one learner can solve all problems; representational design\ndecisions have consequences. b. Sources of error and undecidability in machine\nlearning 3\\. A simple statistical-based supervised learning such as linear\nregression or decision trees a. Focus on how they work without going into\nmathematical or optimization details; enough to understand and use existing\nimplementations correctly 4\\. The overfitting problem/controlling solution\ncomplexity (regularization, pruning - intuition only) a. The bias\n(underfitting) - variance (overfitting) tradeoff 5\\. Working with Data a. Data\npreprocessing i. Importance and pitfalls of preprocessing choices b. Handling\nmissing values (imputing, flag-as-missing) i. Implications of imputing vs\nflag-as-missing c. Encoding categorical variables, encoding real-valued data\nd. Normalization/standardization e. Emphasis on real data, not textbook\nexamples 6\\. Representations a. Hypothesis spaces and complexity b. Simple\nbasis feature expansion, such as squaring univariate features c. Learned\nfeature representations 7\\. Machine learning evaluation a. Separation of\ntrain, validation, and test sets b. Performance metrics for classifiers c.\nEstimation of test performance on held-out data d. Tuning the parameters of a\nmachine learning model with a validation set e. Importance of understanding\nwhat a model is doing, where its pitfalls/shortcomings are, and the\nimplications of its decisions 8\\. Basic neural networks a. Fundamentals of\nunderstanding how neural networks work and their training process, without\ndetails of the calculations b. Basic introduction to generative neural\nnetworks (e.g., large language models) 9\\. Ethics for Machine Learning (See\nalso: SEP-Context) a. Focus on real data, real scenarios, and case studies b.\nDataset/algorithmic/evaluation bias and unintended consequences 72 KA Core:\n10\\. Formulation of simple machine learning as an optimization problem, such\nas least squares linear regression or logistic regression a. Objective\nfunction b. Gradient descent c. Regularization to avoid overfitting\n(mathematical formulation) 11\\. Ensembles of models a. Simple weighted\nmajority combination 12\\. Deep learning a. Deep feed-forward networks\n(intuition only, no mathematics) b. Convolutional neural networks (intuition\nonly, no mathematics) c. Visualization of learned feature representations from\ndeep nets d. Other architectures (generative NN, recurrent NN, transformers,\netc.) 13\\. Performance evaluation a. Other metrics for classification (e.g.,\nerror, precision, recall) b. Performance metrics for regressors c. Confusion\nmatrix d. Cross-validation i. Parameter tuning (grid/random search, via cross-\nvalidation) 14\\. Overview of reinforcement learning methods 15\\. Two or more\napplications of machine learning algorithms a. E.g., medicine and health,\neconomics, vision, natural language, robotics, game play 16\\. Ethics for\nMachine Learning a. Continued focus on real data, real scenarios, and case\nstudies (See also: SEP-Context) b. Privacy (See also: SEP-Privacy) c. Fairness\n(See also: SEP-Privacy) d. Intellectual property e. Explainability Non-core:\n17\\. General statistical-based learning, parameter estimation (maximum\nlikelihood) 18\\. Supervised learning a. Decision trees b. Nearest-neighbor\nclassification and regression c. Learning simple neural networks / multi-layer\nperceptrons d. Linear regression e. Logistic regression f. Support vector\nmachines (SVMs) and kernels g. Gaussian Processes 19\\. Overfitting a. The\ncurse of dimensionality b. Regularization (mathematical computations, L2 and\nL1 regularization) 20\\. Experimental design 73 a. Data preparation (e.g.,\nstandardization, representation, one-hot encoding) b. Hypothesis space c.\nBiases (e.g., algorithmic, search) d. Partitioning data: stratification,\ntraining set, validation set, test set e. Parameter tuning (grid/random\nsearch, via cross-validation) f. Performance evaluation i. Cross-validation\nii. Metric: error, precision, recall, confusion matrix iii. Receiver operating\ncharacteristic (ROC) curve and area under ROC curve 21\\. Bayesian learning\n(Cross-Reference AI/Reasoning Under Uncertainty) a. Naive Bayes and its\nrelationship to linear models b. Bayesian networks c. Prior/posterior d.\nGenerative models 22\\. Deep learning a. Deep feed-forward networks b. Neural\ntangent kernel and understanding neural network training c. Convolutional\nneural networks d. Autoencoders e. Recurrent networks f. Representations and\nknowledge transfer g. Adversarial training and generative adversarial networks\nh. Attention mechanisms 23\\. Representations a. Manually crafted\nrepresentations b. Basis expansion c. Learned representations (e.g., deep\nneural networks) 24\\. Unsupervised learning and clustering a. K-means b.\nGaussian mixture models c. Expectation maximization (EM) d. Self-organizing\nmaps 25\\. Graph analysis (e.g., PageRank) 26\\. Semi-supervised learning 27\\.\nGraphical models (See also: AI-Probability) 28\\. Ensembles a. Weighted\nmajority b. Boosting/bagging c. Random forest d. Gated ensemble 29\\. Learning\ntheory a. General overview of learning theory / why learning works b. VC\ndimension c. Generalization bounds 74 30\\. Reinforcement learning a.\nExploration vs exploitation tradeoff b. Markov decision processes c. Value and\npolicy iteration d. Policy gradient methods e. Deep reinforcement learning f.\nLearning from demonstration and inverse RL 31\\. Explainable / interpretable\nmachine learning a. Understanding feature importance (e.g., LIME, Shapley\nvalues) b. Interpretable models and representations 32\\. Recommender systems\n33\\. Hardware for machine learning a. GPUs / TPUs 34\\. Application of machine\nlearning algorithms to: a. Medicine and health b. Economics c. Education d.\nVision e. Natural language f. Robotics g. Game play h. Data mining (Cross-\nreference DM/Data Analytics) 35\\. Ethics for Machine Learning a. Continued\nfocus on real data, real scenarios, and case studies (See also: SEP-Context)\nb. In depth exploration of dataset/algorithmic/evaluation bias, data privacy,\nand fairness (See also: SEP-Privacy, SEP-Context) c. Trust / explainability\nIllustrative Learning Outcomes: 1\\. Describe the differences among the three\nmain styles of learning (supervised, reinforcement, and unsupervised) and\ndetermine which is appropriate to a particular problem domain. 2\\.\nDifferentiate the terms of AI, machine learning, and deep learning. 3\\. Frame\nan application as a classification problem, including the available input\nfeatures and output to be predicted (e.g., identifying alphabetic characters\nfrom pixel grid input). 4\\. Apply two or more simple statistical learning\nalgorithms to a classification task and measure the classifiers' accuracy. 5\\.\nIdentify overfitting in the context of a problem and learning curves and\ndescribe solutions to overfitting. 6\\. Explain how machine learning works as\nan optimization/search process. 7\\. Implement a statistical learning algorithm\nand the corresponding optimization process to train the classifier and obtain\na prediction on new data. 8\\. Describe the neural network training process and\nresulting learned representations. 75 9\\. Explain proper ML evaluation\nprocedures, including the differences between training and testing\nperformance, and what can go wrong with the evaluation process leading to\ninaccurate reporting of ML performance. 10\\. Compare two machine learning\nalgorithms on a dataset, implementing the data preprocessing and evaluation\nmethodology (e.g., metrics and handling of train/test splits) from scratch.\n11\\. Visualize the training progress of a neural network through learning\ncurves in a well-established toolkit (e.g., TensorBoard) and visualize the\nlearned features of the network. 12\\. Compare and contrast several learning\ntechniques (e.g., decision trees, logistic regression, naive Bayes, neural\nnetworks, and belief networks), providing examples of when each strategy is\nsuperior. 13\\. Evaluate the performance of a simple learning system on a real-\nworld dataset. 14\\. Characterize the state of the art in learning theory,\nincluding its achievements and shortcomings. 15\\. Explain the problem of\noverfitting, along with techniques for detecting and managing the problem.\n16\\. Explain the triple tradeoff among the size of a hypothesis space, the\nsize of the training set, and performance accuracy. 17\\. Given a real-world\napplication of machine learning, describe ethical issues regarding the choices\nof data, preprocessing steps, algorithm selection, and\nvisualization/presentation of results</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/a74ee824765fe42211653006a319ada67bc317f397c20f93ac36db10cc856499> .
_:N6834e785252c4a31b1e479e199bfdc25 <http://schema.org/answer> "0" <http://groq.org/a74ee824765fe42211653006a319ada67bc317f397c20f93ac36db10cc856499> .
_:Nb1731c77dd9340db8727f664005c4f8b <http://example.org/has_source> <file:///home/ilefm/SLM-EXP/data/BodyOfKnowledge/Artificial_Intelligence_AI/AI_Search_Search.txt> <http://faiss.org/cd0dfa83931aceef3af58a8635984aa8a4db03ad30d99359c502563a145bbc7a> .
<http://example.org/faiss> <http://example.org/input> "Label: Optimisation multi-objectif Objectif: (résultats d'apprentissage)\nproblems (M)\n- To be able to apply the e-constraint algorithm with adaptive step to solve a bi-objective\ncombinatorial problem (A)\n- To be able to apply the weighted sum scalarization to compute a complete set of extreme\nsupported solution of a bi-objective combinatorial optimization problem (A)\n- To be able to define and update the search area of a bi-objective combinatorial optimization\nproblem (Phase 2 of the two phase method) (A)\n- To know the main strategies (variable fixing, ranking, branch and bound) to explore the search\narea (A)\n- To know the notion of bound sets (M)\n- To be able to apply a branch and bound algorithm to solve a bi-objective combinatorial\noptimization problem (A)\n- To be able to apply a dynamic programming algorithm for multi-objective combinatorial\noptimization problem, application to the shortest path problem (A) Course content: * Multi-objective combinatorial optimization - main definitions and properties\n* Multi-objective combinatorial optimization - main solution methods\n* Multi-objective shortest path problem Course name: http://example.org/course/UE_XMS3IU300" <http://faiss.org/cd0dfa83931aceef3af58a8635984aa8a4db03ad30d99359c502563a145bbc7a> .
_:N2d3c6fa88e9241b7920d1029c19d8548 <http://example.org/has_source> <file:///home/ilefm/SLM-EXP/data/BodyOfKnowledge/Artificial_Intelligence_AI/AI-ML_Machine_Learning.txt> <http://faiss.org/cd0dfa83931aceef3af58a8635984aa8a4db03ad30d99359c502563a145bbc7a> .
_:N81355ad29a8c4a63be290d61cc8254c0 <http://example.org/has_score> "0.70367074"^^<http://www.w3.org/2001/XMLSchema#float> <http://faiss.org/cd0dfa83931aceef3af58a8635984aa8a4db03ad30d99359c502563a145bbc7a> .
_:Nb1731c77dd9340db8727f664005c4f8b <http://example.org/has_chunk> ". What it means to “solve” a game (e.g., checkers)\n15. Implementation of minimax search, beam search\n16. Expectimax search (MDP-solving) and chance nodes\n17. Stochastic search\na. Simulated annealing\nb. Genetic algorithms\nc. Monte-Carlo tree search\nIllustrative Learning Outcomes:\n1. Design the state space representation for a puzzle (e.g., N-queens or 3-jug problem)\n2. Select and implement an appropriate uninformed search algorithm for a problem (e.g" <http://faiss.org/cd0dfa83931aceef3af58a8635984aa8a4db03ad30d99359c502563a145bbc7a> .
_:N2d3c6fa88e9241b7920d1029c19d8548 <http://example.org/has_chunk> ".\n4. Apply two or more simple statistical learning algorithms to a classification task and measure the\nclassifiers’ accuracy.\n5. Identify overfitting in the context of a problem and learning curves and describe solutions to\noverfitting.\n6. Explain how machine learning works as an optimization/search process.\n7. Implement a statistical learning algorithm and the corresponding optimization process to train the\nclassifier and obtain a prediction on new data.\n8" <http://faiss.org/cd0dfa83931aceef3af58a8635984aa8a4db03ad30d99359c502563a145bbc7a> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/is_aligned_with> _:Nb1731c77dd9340db8727f664005c4f8b <http://faiss.org/cd0dfa83931aceef3af58a8635984aa8a4db03ad30d99359c502563a145bbc7a> .
_:N2d3c6fa88e9241b7920d1029c19d8548 <http://example.org/has_score> "0.7046993"^^<http://www.w3.org/2001/XMLSchema#float> <http://faiss.org/cd0dfa83931aceef3af58a8635984aa8a4db03ad30d99359c502563a145bbc7a> .
_:N81355ad29a8c4a63be290d61cc8254c0 <http://example.org/has_chunk> ".\noptimization, such as instruction selection, instruction scheduling, register allocation, and peephole\noptimization." <http://faiss.org/cd0dfa83931aceef3af58a8635984aa8a4db03ad30d99359c502563a145bbc7a> .
_:N81355ad29a8c4a63be290d61cc8254c0 <http://example.org/has_source> <file:///home/ilefm/SLM-EXP/data/BodyOfKnowledge/Foundations_of_Programming_Languages_FPL/FPL-Code_Code_Generation.txt> <http://faiss.org/cd0dfa83931aceef3af58a8635984aa8a4db03ad30d99359c502563a145bbc7a> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/is_aligned_with> _:N81355ad29a8c4a63be290d61cc8254c0 <http://faiss.org/cd0dfa83931aceef3af58a8635984aa8a4db03ad30d99359c502563a145bbc7a> .
_:Nb1731c77dd9340db8727f664005c4f8b <http://example.org/has_score> "0.72083175"^^<http://www.w3.org/2001/XMLSchema#float> <http://faiss.org/cd0dfa83931aceef3af58a8635984aa8a4db03ad30d99359c502563a145bbc7a> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/is_aligned_with> _:N2d3c6fa88e9241b7920d1029c19d8548 <http://faiss.org/cd0dfa83931aceef3af58a8635984aa8a4db03ad30d99359c502563a145bbc7a> .
<http://example.org/GROK> <http://example.org/param_prompt> "\n        You are a JSON-LD API. Always respond only with a valid JSON-LD object, without explanation or formatting.\n\n        The following describes the content of a lecture and a Knowledge Unit (KU) in Computer Science.\n\n        <page1>Label: Optimisation multi-objectif Objectif: (résultats d'apprentissage)\nproblems (M)\n- To be able to apply the e-constraint algorithm with adaptive step to solve a bi-objective\ncombinatorial problem (A)\n- To be able to apply the weighted sum scalarization to compute a complete set of extreme\nsupported solution of a bi-objective combinatorial optimization problem (A)\n- To be able to define and update the search area of a bi-objective combinatorial optimization\nproblem (Phase 2 of the two phase method) (A)\n- To know the main strategies (variable fixing, ranking, branch and bound) to explore the search\narea (A)\n- To know the notion of bound sets (M)\n- To be able to apply a branch and bound algorithm to solve a bi-objective combinatorial\noptimization problem (A)\n- To be able to apply a dynamic programming algorithm for multi-objective combinatorial\noptimization problem, application to the shortest path problem (A) Course content: * Multi-objective combinatorial optimization - main definitions and properties\n* Multi-objective combinatorial optimization - main solution methods\n* Multi-objective shortest path problem Course name: http://example.org/course/UE_XMS3IU300</page1>\n        <page2>Pages: 142-143 HOURS CS Core = 0 KA Core = 0 FPL-Code: Code Generation Non-\ncore: 1\\. Instruction sets (See also: AR-Assembly) 2\\. Control flow 3\\. Memory\nmanagement (See also: AR-Memory, OS-Memory) 4\\. Procedure calls and method\ndispatching 5\\. Separate compilation; linking 6\\. Instruction selection 7\\.\nInstruction scheduling (e.g., pipelining) 8\\. Register allocation 9\\. Code\noptimization as a form of program analysis (See also: FPL-Analysis) 10\\.\nProgram generation through generative AI Illustrative learning outcomes: Non-\ncore: 1\\. Identify all essential steps for automatically converting source\ncode into assembly or other low-level languages. 2\\. Explain the low-level\ncode necessary for calling functions/methods in modern languages. 3\\. Discuss\nwhy separate compilation requires uniform calling conventions. 4\\. Discuss why\nseparate compilation limits optimization because of unknown effects of calls.\n5\\. Discuss opportunities for optimization introduced by naive translation and\napproaches for achieving. optimization, such as instruction selection,\ninstruction scheduling, register allocation, and peephole optimization.</page2>\n\n        Determine whether the lecture (page1) substantially covers the knowledge described in the KU (page2).\n\n        Respond only using JSON-LD, with the following structure:\n        {\n        \"@context\": \"http://schema.org/\",\n        \"@type\": \"Report\",\n        \"http://schema.org/answer\": \"A\",\n        \"http://schema.org/explain\": \"B\"\n        }\n\n        Replace A with \"1\" if the lecture covers a large subset of the KU, or \"0\" if it does not.\n        Replace B with a **very short explanation** of your decision.\n        " <http://groq.org/ba3023bccb6f212c97a378ebe3c2b688102a0c2839fc31caf5561cad67b50e31> .
_:Necca618540f24c57b89b5609b7663d80 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Report> <http://groq.org/ba3023bccb6f212c97a378ebe3c2b688102a0c2839fc31caf5561cad67b50e31> .
<http://example.org/GROK> <http://example.org/param_uri> "http://example.org/course/UE_XMS3IU300" <http://groq.org/ba3023bccb6f212c97a378ebe3c2b688102a0c2839fc31caf5561cad67b50e31> .
_:Necca618540f24c57b89b5609b7663d80 <http://schema.org/explain> "The lecture covers topics related to multi-objective combinatorial optimization, which is a subset of the knowledge described in the KU, which focuses on code generation and low-level programming." <http://groq.org/ba3023bccb6f212c97a378ebe3c2b688102a0c2839fc31caf5561cad67b50e31> .
<http://example.org/course/UE_XMS3IU300> <http://example.org/has_schema_type> _:Necca618540f24c57b89b5609b7663d80 <http://groq.org/ba3023bccb6f212c97a378ebe3c2b688102a0c2839fc31caf5561cad67b50e31> .
_:Necca618540f24c57b89b5609b7663d80 <http://schema.org/answer> "1" <http://groq.org/ba3023bccb6f212c97a378ebe3c2b688102a0c2839fc31caf5561cad67b50e31> .
<http://example.org/GROK> <http://example.org/param_model> "llama3-8b-8192" <http://groq.org/ba3023bccb6f212c97a378ebe3c2b688102a0c2839fc31caf5561cad67b50e31> .

